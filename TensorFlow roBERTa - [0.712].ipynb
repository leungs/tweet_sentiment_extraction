{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TensorFlow roBERTa + CNN head - LB 0.712"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hello everyone! \n",
    "\n",
    "I'm glade to participate at this competitions and I want to say thanks to everyone for sharing their great starter kernels.\n",
    "This kernel is based on [Chris Deotte](https://www.kaggle.com/cdeotte) [starter kernel](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705). I changed the head after roBERTa using conv and dense layers, I also used LR scheduler.\n",
    "\n",
    "hope it will help you!\n",
    "Happy Kaggling:)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load  data and libraries"
   ]
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn import model_selection\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from transformers import RobertaConfig, TFRobertaPreTrainedModel, TFRobertaMainLayer, TFRobertaModel\n",
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('TF version',tf.__version__)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def read_train():\n",
    "    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "    train['text']=train['text'].astype(str)\n",
    "    train['selected_text']=train['selected_text'].astype(str)\n",
    "    return train\n",
    "\n",
    "def read_test():\n",
    "    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "    test['text']=test['text'].astype(str)\n",
    "    return test\n",
    "\n",
    "def read_submission():\n",
    "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "    return test\n",
    "    \n",
    "train_df = read_train()\n",
    "test_df = read_test()\n",
    "submission_df = read_submission()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str(str1).lower().split()) \n",
    "    b = set(str(str2).lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preproccesing"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "MAX_SEQUENCE_LENGTH = 96\n",
    "pos_offsets = 1\n",
    "PATH = '../input/tf-roberta/'\n",
    "TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "\n",
    "sentiment_dict = {\"positive\": [\"good\", \"happy\", \"love\", \"day\", \"thanks\", \"great\", \"fun\", \"nice\", \"hope\", \"thank\"],\n",
    "                  \"negative\": [\"miss\", \"sad\", \"sorry\", \"bad\", \"hate\", \"sucks\", \"sick\", \"like\", \"feel\", \"bored\"],\n",
    "                  \"neutral\": [\"get\", \"go\", \"day\", \"work\", \"going\", \"quot\", \"lol\", \"got\", \"like\", \"today\"]}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "I. Set up preprocessing and dataset/datagenerator\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def preprocess(tweet, selected_text, sentiment):\n",
    "    \"\"\"\n",
    "    Will be used in tf.data.Dataset.from_generator(...)\n",
    "    \"\"\"\n",
    "    # The original strings have been converted to\n",
    "    # byte strings, so we need to decode it\n",
    "    tweet = tweet.decode('utf-8')\n",
    "    selected_text = selected_text.decode('utf-8')\n",
    "    sentiment = sentiment.decode('utf-8')  \n",
    "    \n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    # tokenize with offsets\n",
    "    enc = TOKENIZER.encode(tweet)\n",
    "    input_ids_orig = enc.ids\n",
    "    offsets = enc.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    target_start = target_idx[0]\n",
    "    target_end = target_idx[-1]\n",
    "    \n",
    "    # add sentiment word frequency\n",
    "    sentiment_frequency = []\n",
    "    pos_fre = 0\n",
    "    neg_fre = 0\n",
    "    neu_fre = 0\n",
    "    for token in enc.tokens:\n",
    "        if token in sentiment_dict[\"positive\"]:\n",
    "            pos_fre += 1\n",
    "        if token in sentiment_dict[\"negative\"]:\n",
    "            neg_fre += 1\n",
    "        if token in sentiment_dict[\"neutral\"]:\n",
    "            neu_fre += 1\n",
    "    sentiment_frequency.append(str(pos_fre))\n",
    "    sentiment_frequency.append(str(neg_fre))\n",
    "    sentiment_frequency.append(str(neu_fre))\n",
    "    enc_sentiment = TOKENIZER.encode(\" \".join(sentiment_frequency))\n",
    "    \n",
    "    \n",
    "    # add and pad data (hardcoded for BERT)\n",
    "    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n",
    "    # sentiment_map = {\n",
    "    #     'positive': 3893,\n",
    "    #     'negative': 4997,\n",
    "    #     'neutral': 8699,\n",
    "    # }\n",
    "    sentiment_map = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "\n",
    "    input_ids = [0] + input_ids_orig + [2] + [2] + [sentiment_map[sentiment]] + [2]\n",
    "    input_type_ids = [0] * 1 + [0] * (len(input_ids_orig) + 4)\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 5)\n",
    "    offsets = [(0, 0)] + offsets + [(0, 0)] * 4\n",
    "    target_start += pos_offsets\n",
    "    target_end += pos_offsets\n",
    "\n",
    "    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        input_type_ids = input_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "    elif padding_length < 0:\n",
    "        input_ids = input_ids[:padding_length - 1] + [2]\n",
    "        attention_mask = attention_mask[:padding_length - 1] + [1]\n",
    "        input_type_ids = input_type_ids[:padding_length - 1] + [0]\n",
    "        offsets = offsets[:padding_length - 1] + [(0, 0)]\n",
    "        if target_start >= MAX_SEQUENCE_LENGTH:\n",
    "            target_start = MAX_SEQUENCE_LENGTH - 1\n",
    "        if target_end >= MAX_SEQUENCE_LENGTH:\n",
    "            target_end = MAX_SEQUENCE_LENGTH - 1\n",
    "\n",
    "    return (\n",
    "        input_ids, attention_mask, input_type_ids, offsets,\n",
    "        target_start, target_end, tweet, selected_text, sentiment,\n",
    "    )\n",
    "\n",
    "class TweetSentimentDataset(tf.data.Dataset):\n",
    "    \n",
    "    OUTPUT_TYPES = (\n",
    "        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n",
    "        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n",
    "        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_SHAPES = (\n",
    "        (MAX_SEQUENCE_LENGTH,),   (MAX_SEQUENCE_LENGTH,), (MAX_SEQUENCE_LENGTH,), \n",
    "        (MAX_SEQUENCE_LENGTH, 2), (),                     (),\n",
    "        (),                       (),                     (),\n",
    "    )\n",
    "    \n",
    "    # AutoGraph will automatically convert Python code to\n",
    "    # Tensorflow graph code. You could also wrap 'preprocess' \n",
    "    # in tf.py_function(..) for arbitrary python code\n",
    "    def _generator(tweet, selected_text, sentiment):\n",
    "        for tw, st, se in zip(tweet, selected_text, sentiment):\n",
    "            yield preprocess(tw, st, se)\n",
    "    \n",
    "    # This dataset object will return a generator\n",
    "    def __new__(cls, tweet, selected_text, sentiment):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=cls.OUTPUT_TYPES,\n",
    "            output_shapes=cls.OUTPUT_SHAPES,\n",
    "            args=(tweet, selected_text, sentiment)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n",
    "        dataset = TweetSentimentDataset(\n",
    "            dataframe.text.values, \n",
    "            dataframe.selected_text.values, \n",
    "            dataframe.sentiment.values\n",
    "        )\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle_buffer_size != -1:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        # d = next(iter(dataset))\n",
    "        # print(\"Writing example in %d\" % (len(dataframe)))\n",
    "        # for i in range(5):\n",
    "        #     print(\"*** Example ***\")\n",
    "        #     print(\"tokens: %s\" % \" \".join(TOKENIZER.encode(d[6].numpy()[i].decode(\"utf-8\")).tokens))\n",
    "        #     print(\"input_ids: %s\" % \" \".join([str(x) for x in d[0].numpy()[i]]))\n",
    "        #     print(\"input_mask: %s\" % \" \".join([str(x) for x in d[1].numpy()[i]]))\n",
    "        #     print(\"segment_ids: %s\" % \" \".join([str(x) for x in d[2].numpy()[i]]))\n",
    "        #     print(\"selected_text: %s\" % d[7].numpy()[i].decode(\"utf-8\"))\n",
    "        #     print(\"idx_start: %d\" % d[4].numpy()[i])\n",
    "        #     print(\"idx_end: %d\" % d[5].numpy()[i])\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "def generate_fold_data(data, num_folds):\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold_num, (train_idx, valid_idx) in enumerate(kfold.split(X=data.text, y=data.sentiment.values)):\n",
    "        if fold_num == 0:\n",
    "            save_data = data.iloc[valid_idx]\n",
    "            save_data[\"kfold\"] = fold_num\n",
    "        else:\n",
    "            _save_data = data.iloc[valid_idx]\n",
    "            _save_data[\"kfold\"] = fold_num\n",
    "            save_data = pd.concat([save_data, _save_data], axis=0)\n",
    "            \n",
    "    save_data = save_data.reset_index(drop=True)\n",
    "    # print(save_data.shape)\n",
    "    # save_data.to_csv(\"train_5folds.csv\", index=False)\n",
    "    return save_data\n",
    "\n",
    "ct = train_df.shape[0]\n",
    "input_ids = np.ones((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "\n",
    "for k in range(train_df.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = TOKENIZER.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = TOKENIZER.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "ct = test_df.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_SEQUENCE_LENGTH),dtype='int32')\n",
    "\n",
    "for k in range(test_df.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "    enc = TOKENIZER.encode(text1)                \n",
    "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train\n",
    "We will skip this stage and load already trained model"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "n_splits = 4"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true,
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "jac = []; VER='v4'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_SEQUENCE_LENGTH))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=5, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "            enc = TOKENIZER.encode(text1)\n",
    "            st = TOKENIZER.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_SEQUENCE_LENGTH))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_SEQUENCE_LENGTH))\n",
    "DISPLAY=1\n",
    "for i in range(5):\n",
    "    print('#'*25)\n",
    "    print('### MODEL %i'%(i+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    model.load_weights('../input/model4/v4-roberta-%i.h5'%i)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/n_splits\n",
    "    preds_end += preds[1]/n_splits"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test_df.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "        enc = TOKENIZER.encode(text1)\n",
    "        st = TOKENIZER.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}