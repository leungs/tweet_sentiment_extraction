{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Complete TensorFlow mixed-precision implementation with Bert\n",
    "\n",
    "*1.use bert in tensorflow 2.1*\n",
    "*2.add the sentiment frquence: positive negative neutral*\n",
    "*3.use lr warmup*"
   ]
  },
  {
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn import model_selection\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options(\n",
    "    {\"auto_mixed_precision\": True})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# read csv files\n",
    "train_df = pd.read_csv('../input/my-data/train_process.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df.loc[:, \"selected_text\"] = test_df.text.values\n",
    "\n",
    "submission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "\n",
    "print(\"train shape =\", train_df.shape)\n",
    "print(\"test shape  =\", test_df.shape)\n",
    "\n",
    "# set some global variables\n",
    "PATH = \"../input/bert-base-uncased/\"\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "TOKENIZER = BertWordPieceTokenizer(f\"{PATH}/vocab.txt\", lowercase=True, add_special_tokens=False)\n",
    "\n",
    "sentiment_dict = {\"positive\": [\"good\", \"happy\", \"love\", \"day\", \"thanks\", \"great\", \"fun\", \"nice\", \"hope\", \"thank\"],\n",
    "                  \"negative\": [\"miss\", \"sad\", \"sorry\", \"bad\", \"hate\", \"sucks\", \"sick\", \"like\", \"feel\", \"bored\"],\n",
    "                  \"neutral\": [\"get\", \"go\", \"day\", \"work\", \"going\", \"quot\", \"lol\", \"got\", \"like\", \"today\"]}\n",
    "\n",
    "# let's take a look at the data\n",
    "train_df.head(10)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "I. Set up preprocessing and dataset/datagenerator\n",
    "```\n"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def preprocess(tweet, selected_text, sentiment):\n",
    "    \"\"\"\n",
    "    Will be used in tf.data.Dataset.from_generator(...)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # The original strings have been converted to\n",
    "    # byte strings, so we need to decode it\n",
    "    tweet = tweet.decode('utf-8')\n",
    "    selected_text = selected_text.decode('utf-8')\n",
    "    sentiment = sentiment.decode('utf-8')\n",
    "\n",
    "    # Clean up the strings a bit\n",
    "    tweet = \" \".join(str(tweet).lower().split())\n",
    "    selected_text = \" \".join(str(selected_text).lower().split())\n",
    "\n",
    "    tokens_a = TOKENIZER.encode(tweet).tokens\n",
    "\n",
    "    # find the intersection between text and selected text\n",
    "    target_start, target_end = None, None\n",
    "    if selected_text is not None:\n",
    "        selected_texts_a = TOKENIZER.encode(selected_text).tokens\n",
    "        # find the intersection between text and selected text\n",
    "        for index in (i for i, c in enumerate(tokens_a) if c == selected_texts_a[0]):\n",
    "            if tokens_a[index:index + len(selected_texts_a)] == selected_texts_a:\n",
    "                target_start = index\n",
    "                target_end = index + len(selected_texts_a)\n",
    "                break\n",
    "\n",
    "    # tokenize with offsets\n",
    "    enc = TOKENIZER.encode(tweet)\n",
    "    input_ids_orig, offsets = enc.ids, enc.offsets\n",
    "    \n",
    "    # add sentiment word frequency\n",
    "    sentiment_frequency = []\n",
    "    pos_fre = 0\n",
    "    neg_fre = 0\n",
    "    neu_fre = 0\n",
    "    for token in enc.tokens:\n",
    "        if token in sentiment_dict[\"positive\"]:\n",
    "            pos_fre += 1\n",
    "        if token in sentiment_dict[\"negative\"]:\n",
    "            neg_fre += 1\n",
    "        if token in sentiment_dict[\"neutral\"]:\n",
    "            neu_fre += 1\n",
    "    sentiment_frequency.append(str(pos_fre))\n",
    "    sentiment_frequency.append(str(neg_fre))\n",
    "    sentiment_frequency.append(str(neu_fre))\n",
    "    enc_sentiment = TOKENIZER.encode(\" \".join(sentiment_frequency))\n",
    "    \n",
    "    \n",
    "    # add and pad data (hardcoded for BERT)\n",
    "    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n",
    "    sentiment_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699,\n",
    "    }\n",
    "\n",
    "    input_ids = [101] + [sentiment_map[sentiment]] + enc_sentiment.ids + [102] + input_ids_orig + [102]\n",
    "    input_type_ids = [0, 0, 0, 0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 7)\n",
    "    offsets = [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)] + offsets + [(0, 0)]\n",
    "    target_start += 6\n",
    "    target_end += 5\n",
    "\n",
    "    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        input_type_ids = input_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "    elif padding_length < 0:\n",
    "        input_ids = input_ids[:padding_length - 1] + [102]\n",
    "        attention_mask = attention_mask[:padding_length - 1] + [1]\n",
    "        input_type_ids = input_type_ids[:padding_length - 1] + [1]\n",
    "        offsets = offsets[:padding_length - 1] + [(0, 0)]\n",
    "        if target_start >= MAX_SEQUENCE_LENGTH:\n",
    "            target_start = MAX_SEQUENCE_LENGTH - 1\n",
    "        if target_end >= MAX_SEQUENCE_LENGTH:\n",
    "            target_end = MAX_SEQUENCE_LENGTH - 1\n",
    "\n",
    "    return (\n",
    "        input_ids, attention_mask, input_type_ids, offsets,\n",
    "        target_start, target_end, tweet, selected_text, sentiment,\n",
    "    )\n",
    "\n",
    "class TweetSentimentDataset(tf.data.Dataset):\n",
    "    \n",
    "    OUTPUT_TYPES = (\n",
    "        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n",
    "        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n",
    "        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_SHAPES = (\n",
    "        (MAX_SEQUENCE_LENGTH,),   (MAX_SEQUENCE_LENGTH,), (MAX_SEQUENCE_LENGTH,), \n",
    "        (MAX_SEQUENCE_LENGTH, 2), (),                     (),\n",
    "        (),                       (),                     (),\n",
    "    )\n",
    "    \n",
    "    # AutoGraph will automatically convert Python code to\n",
    "    # Tensorflow graph code. You could also wrap 'preprocess' \n",
    "    # in tf.py_function(..) for arbitrary python code\n",
    "    def _generator(tweet, selected_text, sentiment):\n",
    "        for tw, st, se in zip(tweet, selected_text, sentiment):\n",
    "            yield preprocess(tw, st, se)\n",
    "    \n",
    "    # This dataset object will return a generator\n",
    "    def __new__(cls, tweet, selected_text, sentiment):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=cls.OUTPUT_TYPES,\n",
    "            output_shapes=cls.OUTPUT_SHAPES,\n",
    "            args=(tweet, selected_text, sentiment)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n",
    "        dataset = TweetSentimentDataset(\n",
    "            dataframe.text.values, \n",
    "            dataframe.selected_text.values, \n",
    "            dataframe.sentiment.values\n",
    "        )\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle_buffer_size != -1:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        # d = next(iter(dataset))\n",
    "        # print(\"Writing example in %d\" % (len(dataframe)))\n",
    "        # for i in range(5):\n",
    "        #     print(\"*** Example ***\")\n",
    "        #     print(\"tokens: %s\" % \" \".join(TOKENIZER.encode(d[6].numpy()[i].decode(\"utf-8\")).tokens))\n",
    "        #     print(\"input_ids: %s\" % \" \".join([str(x) for x in d[0].numpy()[i]]))\n",
    "        #     print(\"input_mask: %s\" % \" \".join([str(x) for x in d[1].numpy()[i]]))\n",
    "        #     print(\"segment_ids: %s\" % \" \".join([str(x) for x in d[2].numpy()[i]]))\n",
    "        #     print(\"selected_text: %s\" % d[7].numpy()[i].decode(\"utf-8\"))\n",
    "        #     print(\"idx_start: %d\" % d[4].numpy()[i])\n",
    "        #     print(\"idx_end: %d\" % d[5].numpy()[i])\n",
    "        \n",
    "        return dataset\n",
    "        "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n```\nII. Set up transformer model and functions\n```"
  },
  {
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "class BertQAModel(TFBertPreTrainedModel):\n    \n    DROPOUT_RATE = 0.1\n    NUM_HIDDEN_STATES = 2\n    \n    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        \n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        self.concat = L.Concatenate()\n        self.dropout = L.Dropout(self.DROPOUT_RATE)\n        self.qa_outputs = L.Dense(\n            config.num_labels, \n            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n            dtype='float32',\n            name=\"qa_outputs\")\n        \n    @tf.function\n    def call(self, inputs, **kwargs):\n        # outputs: Tuple[sequence, pooled, hidden_states]\n        _, _, hidden_states = self.bert(inputs, **kwargs)\n        \n        hidden_states = self.concat([\n            hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n        ])\n        \n        hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n        logits = self.qa_outputs(hidden_states)\n        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n        start_logits = tf.squeeze(start_logits, axis=-1)\n        end_logits = tf.squeeze(end_logits, axis=-1)\n        \n        return start_logits, end_logits\n    \n    \ndef train(model, dataset, loss_fn, optimizer, current_step):\n    \n    @tf.function\n    def train_step(model, inputs, y_true, loss_fn, optimizer, current_step):\n        with tf.GradientTape() as tape:\n            y_pred = model(inputs, training=True)\n            loss  = loss_fn(y_true[0], y_pred[0])\n            loss += loss_fn(y_true[1], y_pred[1])\n            scaled_loss = optimizer.get_scaled_loss(loss)\n    \n        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n        # optimizer.learning_rate = learning_rate_decay(learning_rate, num_train_steps, num_warmup_steps, current_step)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, y_pred\n\n    epoch_loss = 0.\n    for batch_num, sample in enumerate(dataset):\n        current_step.assign_add(1)\n        loss, y_pred = train_step(\n            model, sample[:3], sample[4:6], loss_fn, optimizer, current_step)\n\n        epoch_loss += loss\n\n        print(\n            f\"training ... batch {batch_num+1:03d} : \"\n            f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n            end='\\r')\n        \ndef predict(model, dataset, loss_fn, optimizer):\n    \n    @tf.function\n    def predict_step(model, inputs):\n        return model(inputs)\n        \n    def to_numpy(*args):\n        out = []\n        for arg in args:\n            if arg.dtype == tf.string:\n                arg = [s.decode('utf-8') for s in arg.numpy()]\n                out.append(arg)\n            else:\n                arg = arg.numpy()\n                out.append(arg)\n        return out\n    \n    # Initialize accumulators\n    offset = tf.zeros([0, MAX_SEQUENCE_LENGTH, 2], dtype=tf.dtypes.int32)\n    text = tf.zeros([0,], dtype=tf.dtypes.string)\n    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n    pred_start = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n    pred_end = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n    \n    for batch_num, sample in enumerate(dataset):\n        \n        print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n        \n        y_pred = predict_step(model, sample[:3])\n        \n        # add batch to accumulators\n        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n        offset = tf.concat((offset, sample[3]), axis=0)\n        text = tf.concat((text, sample[6]), axis=0)\n        selected_text = tf.concat((selected_text, sample[7]), axis=0)\n        sentiment = tf.concat((sentiment, sample[8]), axis=0)\n\n    # pred_start = tf.nn.softmax(pred_start)\n    # pred_end = tf.nn.softmax(pred_end)\n    \n    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n    \n    return pred_start, pred_end, text, selected_text, sentiment, offset\n\n\ndef decode_prediction(pred_start, pred_end, text, offset, sentiment):\n    \n    def decode(pred_start, pred_end, text, offset):\n\n        decoded_text = \"\"\n        for i in range(pred_start, pred_end+1):\n            decoded_text += text[offset[i][0]:offset[i][1]]\n            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n                decoded_text += \" \"\n        return decoded_text\n    \n    decoded_predictions = []\n    for i in range(len(text)):\n        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n            decoded_text = text[i]\n        else:\n            idx_start = np.argmax(pred_start[i])\n            idx_end = np.argmax(pred_end[i])\n            if idx_start > idx_end:\n                idx_end = idx_start \n            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n            if len(decoded_text) == 0:\n                decoded_text = text[i]\n        decoded_predictions.append(decoded_text)\n    \n    return decoded_predictions\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n@tf.function\ndef learning_rate_decay(init_lr, num_train_steps, num_warmup_steps, current_step):\n    # Implements linear decay of the learning rate.\n    learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(\n                    init_lr, num_train_steps, end_learning_rate=0.0, power=1.0)(current_step)\n\n    if num_warmup_steps:\n        global_steps_int = tf.cast(current_step, tf.dtypes.int32)\n        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.dtypes.int32)\n\n        global_steps_float = tf.cast(global_steps_int, tf.dtypes.float32)\n        warmup_steps_float = tf.cast(warmup_steps_int, tf.dtypes.float32)\n\n        warmup_percent_done = global_steps_float / warmup_steps_float\n        warmup_learning_rate = init_lr * warmup_percent_done\n\n        if global_steps_int < warmup_steps_int:\n            learning_rate = warmup_learning_rate\n        else:\n            learning_rate = learning_rate\n        \n    return learning_rate",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "```\nIII. Run it all: \n\nmodel.create() -> dataset.create() -> train(train) ->\n       -> predict(val).decode() -> predict(test).decode() -> submit\n```"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "num_folds = 4\nnum_epochs = 3\nbatch_size = 32\nlearning_rate = 3e-5\nnum_train_steps = int(len(train_df) / batch_size * num_epochs)\nnum_warmup_steps = int(num_train_steps * 0.1)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate)\noptimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n    optimizer, 'dynamic')\n\nconfig = BertConfig(output_hidden_states=True, num_labels=2)\nBertQAModel.DROPOUT_RATE = 0.2\nBertQAModel.NUM_HIDDEN_STATES = 2\nmodel = BertQAModel.from_pretrained(PATH, config=config)\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nkfold = model_selection.StratifiedKFold(\n    n_splits=num_folds, shuffle=True, random_state=42)\n\n# initialize test predictions\ntest_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\ntest_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n\nfor fold_num, (train_idx, valid_idx) in enumerate(\n        kfold.split(X=train_df.text, y=train_df.sentiment.values)):\n    print(\"\\nfold %02d\" % (fold_num+1))\n    \n    global_step = tf.Variable(0, name=\"global_step\")\n    train_dataset = TweetSentimentDataset.create(\n        train_df.iloc[train_idx], batch_size, shuffle_buffer_size=2048)\n    valid_dataset = TweetSentimentDataset.create(\n        train_df.iloc[valid_idx], batch_size, shuffle_buffer_size=-1)\n    test_dataset = TweetSentimentDataset.create(\n        test_df, batch_size, shuffle_buffer_size=-1)\n    \n    best_score = float('-inf')\n    for epoch_num in range(num_epochs):\n        print(\"\\nepoch %03d\" % (epoch_num+1))\n        # train for an epoch\n        train(model, train_dataset, loss_fn, optimizer, global_step)\n        \n        # predict validation set and compute jaccardian distances\n        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n            predict(model, valid_dataset, loss_fn, optimizer)\n        \n        selected_text_pred = decode_prediction(\n            pred_start, pred_end, text, offset, sentiment)\n        jaccards = []\n        for i in range(len(selected_text)):\n            jaccards.append(\n                jaccard(selected_text[i], selected_text_pred[i]))\n        \n        score = np.mean(jaccards)\n        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n        \n        if score > best_score:\n            best_score = score\n            # requires you to have 'fold-{fold_num}' folder in PATH:\n            # model.save_pretrained(PATH+f'fold-{fold_num}')\n            # or\n            # model.save_weights(PATH + f'fold-{fold_num}.h5')\n            \n            # predict test set\n            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n                predict(model, test_dataset, loss_fn, optimizer)\n    \n    # add epoch's best test preds to test preds arrays\n    test_preds_start += test_pred_start\n    test_preds_end += test_pred_end\n    \n    # reset model, as well as session and graph (to avoid OOM issues?) \n    session = tf.compat.v1.get_default_session()\n    graph = tf.compat.v1.get_default_graph()\n    del session, graph, model\n    model = BertQAModel.from_pretrained(PATH, config=config)\n\n    \n# decode test set and add to submission file\nselected_text_pred = decode_prediction(\n    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n\nsubmission_df.loc[:, 'selected_text'] = selected_text_pred\nsubmission_df.to_csv(\"submission.csv\", index=False)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}