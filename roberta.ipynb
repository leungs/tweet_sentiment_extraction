{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete TensorFlow mixed-precision implementation with Bert\n",
    "\n",
    "-*1.use roberta in tensorflow 2.1*\n",
    "-*2.add the sentiment frquence: positive negative neutral*\n",
    "-*3.use lr warmup*\n",
    "-*4.expand data in word_counts.py*\n",
    "-*5.use joblib.Parallel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn import model_selection\n",
    "# from transformers import RobertaConfig, TFRobertaPreTrainedModel, TFRobertaMainLayer\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options(\n",
    "    {\"auto_mixed_precision\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (27480, 4)\n",
      "test shape  = (3534, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>sooo sad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>sons of * * * * ,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>wow . . . u just became cooler .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           sooo sad  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                  sons of * * * * ,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                   wow . . . u just became cooler .  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv files\n",
    "train_df = pd.read_csv('../input/my-data/train_process.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df.loc[:, \"selected_text\"] = test_df.text.values\n",
    "\n",
    "submission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "\n",
    "print(\"train shape =\", train_df.shape)\n",
    "print(\"test shape  =\", test_df.shape)\n",
    "\n",
    "# # merge word_counts_prediction.csv\n",
    "# word_counts_prediction = pd.read_csv(\"../input/my-data/word_counts_prediction.csv\")\n",
    "# train_df = pd.merge(train_df, word_counts_prediction, on=\"textID\")\n",
    "# test_df = pd.merge(test_df, word_counts_prediction, on=\"textID\")\n",
    "\n",
    "# set some global variables\n",
    "PATH = \"../input/huggingfacetransformermodels/model_classes/roberta/roberta-large-tf2-model/\"\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "TOKENIZER = BertWordPieceTokenizer(f\"../input/bert-base-uncased/vocab.txt\", lowercase=True, add_special_tokens=False)\n",
    "\n",
    "sentiment_dict = {\"positive\": [\"good\", \"happy\", \"love\", \"day\", \"thanks\", \"great\", \"fun\", \"nice\", \"hope\", \"thank\"],\n",
    "                  \"negative\": [\"miss\", \"sad\", \"sorry\", \"bad\", \"hate\", \"sucks\", \"sick\", \"like\", \"feel\", \"bored\"],\n",
    "                  \"neutral\": [\"get\", \"go\", \"day\", \"work\", \"going\", \"quot\", \"lol\", \"got\", \"like\", \"today\"]}\n",
    "\n",
    "# let's take a look at the data\n",
    "train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "I. Set up preprocessing and dataset/datagenerator\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet, selected_text, sentiment):\n",
    "    \"\"\"\n",
    "    Will be used in tf.data.Dataset.from_generator(...)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # The original strings have been converted to\n",
    "    # byte strings, so we need to decode it\n",
    "    tweet = tweet.decode('utf-8')\n",
    "    selected_text = selected_text.decode('utf-8')\n",
    "    sentiment = sentiment.decode('utf-8')\n",
    "    # predicted_selection = predicted_selection.decode('utf-8')\n",
    "\n",
    "    # Clean up the strings a bit\n",
    "    tweet = \" \".join(str(tweet).lower().split())\n",
    "    selected_text = \" \".join(str(selected_text).lower().split())\n",
    "    # predicted_selection = \" \".join(str(predicted_selection).lower().split())\n",
    "\n",
    "    tokens_a = TOKENIZER.encode(tweet).tokens\n",
    "\n",
    "    # find the intersection between text and selected text\n",
    "    target_start, target_end = None, None\n",
    "    if selected_text is not None:\n",
    "        selected_texts_a = TOKENIZER.encode(selected_text).tokens\n",
    "        # find the intersection between text and selected text\n",
    "        for index in (i for i, c in enumerate(tokens_a) if c == selected_texts_a[0]):\n",
    "            if tokens_a[index:index + len(selected_texts_a)] == selected_texts_a:\n",
    "                target_start = index\n",
    "                target_end = index + len(selected_texts_a)\n",
    "                break\n",
    "\n",
    "    # tokenize with offsets\n",
    "    enc = TOKENIZER.encode(tweet)\n",
    "    input_ids_orig, offsets = enc.ids, enc.offsets\n",
    "    \n",
    "    # add sentiment word frequency\n",
    "    sentiment_frequency = []\n",
    "    pos_fre = 0\n",
    "    neg_fre = 0\n",
    "    neu_fre = 0\n",
    "    for token in enc.tokens:\n",
    "        if token in sentiment_dict[\"positive\"]:\n",
    "            pos_fre += 1\n",
    "        if token in sentiment_dict[\"negative\"]:\n",
    "            neg_fre += 1\n",
    "        if token in sentiment_dict[\"neutral\"]:\n",
    "            neu_fre += 1\n",
    "    sentiment_frequency.append(str(pos_fre))\n",
    "    sentiment_frequency.append(str(neg_fre))\n",
    "    sentiment_frequency.append(str(neu_fre))\n",
    "    enc_sentiment = TOKENIZER.encode(\" \".join(sentiment_frequency))\n",
    "    \n",
    "    # # add predicted_selection\n",
    "    # enc_ps = TOKENIZER.encode(predicted_selection)\n",
    "    # input_ids_orig_ps, tokens_ps = enc_ps.ids, enc_ps.tokens\n",
    "    \n",
    "    # add and pad data (hardcoded for BERT)\n",
    "    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n",
    "    sentiment_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699,\n",
    "    }\n",
    "\n",
    "    input_ids = [101] + [sentiment_map[sentiment]] + enc_sentiment.ids + [102] + input_ids_orig + [102]\n",
    "    input_type_ids = [0, 0, 0, 0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 7)\n",
    "    offsets = [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)] + offsets + [(0, 0)]\n",
    "    target_start += 6\n",
    "    target_end += 5\n",
    "\n",
    "    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        input_type_ids = input_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "    elif padding_length < 0:\n",
    "        input_ids = input_ids[:padding_length - 1] + [102]\n",
    "        attention_mask = attention_mask[:padding_length - 1] + [1]\n",
    "        input_type_ids = input_type_ids[:padding_length - 1] + [1]\n",
    "        offsets = offsets[:padding_length - 1] + [(0, 0)]\n",
    "        if target_start >= MAX_SEQUENCE_LENGTH:\n",
    "            target_start = MAX_SEQUENCE_LENGTH - 1\n",
    "        if target_end >= MAX_SEQUENCE_LENGTH:\n",
    "            target_end = MAX_SEQUENCE_LENGTH - 1\n",
    "\n",
    "    return (\n",
    "        input_ids, attention_mask, input_type_ids, offsets,\n",
    "        target_start, target_end, tweet, selected_text, sentiment,\n",
    "        # predicted_selection,\n",
    "    )\n",
    "\n",
    "class TweetSentimentDataset(tf.data.Dataset):\n",
    "    \n",
    "    OUTPUT_TYPES = (\n",
    "        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n",
    "        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n",
    "        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n",
    "        # tf.dtypes.string,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_SHAPES = (\n",
    "        (MAX_SEQUENCE_LENGTH,),   (MAX_SEQUENCE_LENGTH,), (MAX_SEQUENCE_LENGTH,), \n",
    "        (MAX_SEQUENCE_LENGTH, 2), (),                     (),\n",
    "        (),                       (),                     (),\n",
    "        # (),\n",
    "    )\n",
    "    \n",
    "    # AutoGraph will automatically convert Python code to\n",
    "    # Tensorflow graph code. You could also wrap 'preprocess' \n",
    "    # in tf.py_function(..) for arbitrary python code\n",
    "    def _generator(tweet, selected_text, sentiment):\n",
    "        for tw, st, se in zip(tweet, selected_text, sentiment):\n",
    "            yield preprocess(tw, st, se)\n",
    "    \n",
    "    # This dataset object will return a generator\n",
    "    def __new__(cls, tweet, selected_text, sentiment):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=cls.OUTPUT_TYPES,\n",
    "            output_shapes=cls.OUTPUT_SHAPES,\n",
    "            args=(tweet, selected_text, sentiment)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n",
    "        dataset = TweetSentimentDataset(\n",
    "            dataframe.text.values, \n",
    "            dataframe.selected_text.values, \n",
    "            dataframe.sentiment.values,\n",
    "            # dataframe.predicted_selection.values,\n",
    "        )\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle_buffer_size != -1:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        # d = next(iter(dataset))\n",
    "        # print(\"Writing example in %d\" % (len(dataframe)))\n",
    "        # for i in range(5):\n",
    "        #     print(\"*** Example ***\")\n",
    "        #     print(\"tokens: %s\" % \" \".join(TOKENIZER.encode(d[6].numpy()[i].decode(\"utf-8\")).tokens))\n",
    "        #     print(\"tokens_predicted_selection: %s\" % \" \".join(TOKENIZER.encode(d[9].numpy()[i].decode(\"utf-8\")).tokens))\n",
    "        #     print(\"input_ids: %s\" % \" \".join([str(x) for x in d[0].numpy()[i]]))\n",
    "        #     print(\"input_mask: %s\" % \" \".join([str(x) for x in d[1].numpy()[i]]))\n",
    "        #     print(\"segment_ids: %s\" % \" \".join([str(x) for x in d[2].numpy()[i]]))\n",
    "        #     print(\"selected_text: %s\" % d[7].numpy()[i].decode(\"utf-8\"))\n",
    "        #     print(\"idx_start: %d\" % d[4].numpy()[i])\n",
    "        #     print(\"idx_end: %d\" % d[5].numpy()[i])\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "def generate_fold_data(data, num_folds):\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold_num, (train_idx, valid_idx) in enumerate(kfold.split(X=data.text, y=data.sentiment.values)):\n",
    "        if fold_num == 0:\n",
    "            save_data = data.iloc[valid_idx]\n",
    "            save_data[\"kfold\"] = fold_num\n",
    "        else:\n",
    "            _save_data = data.iloc[valid_idx]\n",
    "            _save_data[\"kfold\"] = fold_num\n",
    "            save_data = pd.concat([save_data, _save_data], axis=0)\n",
    "            \n",
    "    save_data = save_data.reset_index(drop=True)\n",
    "    # print(save_data.shape)\n",
    "    # save_data.to_csv(\"train_5folds.csv\", index=False)\n",
    "    return save_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "II. Set up transformer model and functions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class RoBertQAModel(TFRobertaPreTrainedModel):\n",
    "class BertQAModel(TFBertPreTrainedModel):\n",
    "    \n",
    "    DROPOUT_RATE = 0.1\n",
    "    NUM_HIDDEN_STATES = 2\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "        # self.robert = TFRobertaMainLayer(config, name=\"robert\")\n",
    "        self.robert = TFBertMainLayer(config, name=\"robert\")\n",
    "        self.concat = L.Concatenate()\n",
    "        self.dropout = L.Dropout(self.DROPOUT_RATE)\n",
    "        self.qa_outputs = L.Dense(\n",
    "            config.num_labels, \n",
    "            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "            dtype='float32',\n",
    "            name=\"qa_outputs\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # outputs: Tuple[sequence, pooled, hidden_states]\n",
    "        _, _, hidden_states = self.robert(inputs, **kwargs)\n",
    "        \n",
    "        hidden_states = self.concat([\n",
    "            hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n",
    "        ])\n",
    "        \n",
    "        hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    \n",
    "def train(model, dataset, loss_fn, optimizer, current_step, loss_step, data_len, fold_num):\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(model, inputs, y_true, loss_fn, optimizer, current_step):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(inputs, training=True)\n",
    "            loss  = loss_fn(y_true[0], y_pred[0])\n",
    "            loss += loss_fn(y_true[1], y_pred[1])\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    \n",
    "        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        # optimizer.learning_rate = learning_rate_decay(learning_rate, num_train_steps, num_warmup_steps, current_step)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss, y_pred\n",
    "\n",
    "    epoch_loss = 0.\n",
    "    tk0 = tqdm(dataset, total=data_len, desc=\"Training: \" + str(fold_num))\n",
    "    for batch_num, sample in enumerate(tk0):\n",
    "        current_step.assign_add(1)\n",
    "        loss, y_pred = train_step(\n",
    "            model, sample[:3], sample[4:6], loss_fn, optimizer, current_step)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        loss_step.append(epoch_loss/(batch_num+1))\n",
    "        # print(\n",
    "        #     f\"training ... batch {batch_num+1:03d} : \"\n",
    "        #     f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n",
    "        #     end='\\r')\n",
    "        \n",
    "def predict(model, dataset, loss_fn, optimizer, data_len, fold_num):\n",
    "    \n",
    "    @tf.function\n",
    "    def predict_step(model, inputs):\n",
    "        return model(inputs)\n",
    "        \n",
    "    def to_numpy(*args):\n",
    "        out = []\n",
    "        for arg in args:\n",
    "            if arg.dtype == tf.string:\n",
    "                arg = [s.decode('utf-8') for s in arg.numpy()]\n",
    "                out.append(arg)\n",
    "            else:\n",
    "                arg = arg.numpy()\n",
    "                out.append(arg)\n",
    "        return out\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    offset = tf.zeros([0, MAX_SEQUENCE_LENGTH, 2], dtype=tf.dtypes.int32)\n",
    "    text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    pred_start = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n",
    "    pred_end = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n",
    "    \n",
    "    tk0 = tqdm(dataset, total=data_len, desc=\"Validating or Testing: \" + str(fold_num))\n",
    "    for batch_num, sample in enumerate(tk0):\n",
    "        \n",
    "        # print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n",
    "        \n",
    "        y_pred = predict_step(model, sample[:3])\n",
    "        \n",
    "        # add batch to accumulators\n",
    "        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n",
    "        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n",
    "        offset = tf.concat((offset, sample[3]), axis=0)\n",
    "        text = tf.concat((text, sample[6]), axis=0)\n",
    "        selected_text = tf.concat((selected_text, sample[7]), axis=0)\n",
    "        sentiment = tf.concat((sentiment, sample[8]), axis=0)\n",
    "\n",
    "    # pred_start = tf.nn.softmax(pred_start)\n",
    "    # pred_end = tf.nn.softmax(pred_end)\n",
    "    \n",
    "    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n",
    "    \n",
    "    return pred_start, pred_end, text, selected_text, sentiment, offset\n",
    "\n",
    "\n",
    "def decode_prediction(pred_start, pred_end, text, offset, sentiment):\n",
    "    \n",
    "    def decode(pred_start, pred_end, text, offset):\n",
    "\n",
    "        decoded_text = \"\"\n",
    "        for i in range(pred_start, pred_end+1):\n",
    "            decoded_text += text[offset[i][0]:offset[i][1]]\n",
    "            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n",
    "                decoded_text += \" \"\n",
    "        return decoded_text\n",
    "    \n",
    "    decoded_predictions = []\n",
    "    for i in range(len(text)):\n",
    "        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n",
    "            decoded_text = text[i]\n",
    "        else:\n",
    "            idx_start = np.argmax(pred_start[i])\n",
    "            idx_end = np.argmax(pred_end[i])\n",
    "            if idx_start > idx_end:\n",
    "                idx_end = idx_start \n",
    "            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n",
    "            if len(decoded_text) == 0:\n",
    "                decoded_text = text[i]\n",
    "        decoded_predictions.append(decoded_text)\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "@tf.function\n",
    "def learning_rate_decay(init_lr, num_train_steps, num_warmup_steps, current_step):\n",
    "    # Implements linear decay of the learning rate.\n",
    "    learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                    init_lr, num_train_steps, end_learning_rate=0.0, power=1.0)(current_step)\n",
    "\n",
    "    if num_warmup_steps:\n",
    "        global_steps_int = tf.cast(current_step, tf.dtypes.int32)\n",
    "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.dtypes.int32)\n",
    "\n",
    "        global_steps_float = tf.cast(global_steps_int, tf.dtypes.float32)\n",
    "        warmup_steps_float = tf.cast(warmup_steps_int, tf.dtypes.float32)\n",
    "\n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "        if global_steps_int < warmup_steps_int:\n",
    "            learning_rate = warmup_learning_rate\n",
    "        else:\n",
    "            learning_rate = learning_rate\n",
    "        \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "III. Run it all: \n",
    "\n",
    "model.create() -> dataset.create() -> train(train) ->\n",
    "       -> predict(val).decode() -> predict(test).decode() -> submit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "num_folds = 5\n",
    "num_epochs = 3\n",
    "batch_size = 50\n",
    "learning_rate = 4e-5\n",
    "num_train_steps = int(len(train_df) / batch_size * num_epochs)\n",
    "num_warmup_steps = int(num_train_steps * 0.1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "    optimizer, 'dynamic')\n",
    "\n",
    "config = RobertaConfig(output_hidden_states=True, num_labels=2)\n",
    "RoBertQAModel.DROPOUT_RATE = 0.2\n",
    "RoBertQAModel.NUM_HIDDEN_STATES = 2\n",
    "model = RoBertQAModel.from_pretrained(PATH, config=config)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(\n",
    "    n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# initialize test predictions\n",
    "test_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "test_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(\n",
    "        kfold.split(X=train_df.text, y=train_df.sentiment.values)):\n",
    "    print(\"\\nfold %02d\" % (fold_num+1))\n",
    "    \n",
    "    loss_step = []\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "    train_dataset = TweetSentimentDataset.create(\n",
    "        train_df.iloc[train_idx], batch_size, shuffle_buffer_size=2048)\n",
    "    valid_dataset = TweetSentimentDataset.create(\n",
    "        train_df.iloc[valid_idx], batch_size, shuffle_buffer_size=-1)\n",
    "    test_dataset = TweetSentimentDataset.create(\n",
    "        test_df, batch_size, shuffle_buffer_size=-1)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    for epoch_num in range(num_epochs):\n",
    "        print(\"\\nepoch %03d\" % (epoch_num+1))\n",
    "        # train for an epoch\n",
    "        train(model, train_dataset, loss_fn, optimizer, global_step, loss_step)\n",
    "        \n",
    "        plt.plot(list(range(global_step.numpy())), loss_step)\n",
    "        plt.show()\n",
    "        \n",
    "        # predict validation set and compute jaccardian distances\n",
    "        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "            predict(model, valid_dataset, loss_fn, optimizer)\n",
    "        \n",
    "        selected_text_pred = decode_prediction(\n",
    "            pred_start, pred_end, text, offset, sentiment)\n",
    "        jaccards = []\n",
    "        for i in range(len(selected_text)):\n",
    "            jaccards.append(\n",
    "                jaccard(selected_text[i], selected_text_pred[i]))\n",
    "        \n",
    "        score = np.mean(jaccards)\n",
    "        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            # requires you to have 'fold-{fold_num}' folder in PATH:\n",
    "            # model.save_pretrained(PATH+f'fold-{fold_num}')\n",
    "            # or\n",
    "            # model.save_weights(PATH + f'fold-{fold_num}.h5')\n",
    "            \n",
    "            # predict test set\n",
    "            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n",
    "                predict(model, test_dataset, loss_fn, optimizer)\n",
    "    \n",
    "    # add epoch's best test preds to test preds arrays\n",
    "    test_preds_start += test_pred_start\n",
    "    test_preds_end += test_pred_end\n",
    "    \n",
    "    # reset model, as well as session and graph (to avoid OOM issues?) \n",
    "    session = tf.compat.v1.get_default_session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    del session, graph, model\n",
    "    model = RoBertQAModel.from_pretrained(PATH, config=config)\n",
    "    break\n",
    "    \n",
    "# decode test set and add to submission file\n",
    "selected_text_pred = decode_prediction(\n",
    "    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n",
    "\n",
    "submission_df.loc[:, 'selected_text'] = selected_text_pred\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 01\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6184750805655036               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6207123118625367               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.618340574200468               \n",
      "\n",
      "fold 02\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6145207552234808               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.624140734730702               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6130763425641519               \n",
      "\n",
      "fold 03\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6064579633537712               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6204347494436645               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6253786957909664               \n",
      "predicting ... batch 111                    \r\n",
      "fold 04\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6266902699490801               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6323951274141497               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6245715136336145               \n"
     ]
    }
   ],
   "source": [
    "num_folds = 1\n",
    "num_epochs = 3\n",
    "batch_size = 50\n",
    "learning_rate = 4e-5\n",
    "num_train_steps = int(len(train_df) / batch_size * num_epochs)\n",
    "num_warmup_steps = int(num_train_steps * 0.1)\n",
    "    \n",
    "data_df_5folds = generate_fold_data(train_df, 4)\n",
    "\n",
    "def run(fold):\n",
    "    df_train_fold = data_df_5folds[data_df_5folds.kfold != fold].reset_index(drop=True)\n",
    "    df_valid_fold = data_df_5folds[data_df_5folds.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    num_train_batches = len(df_train_fold) // batch_size + int(len(df_train_fold) % batch_size != 0)\n",
    "    num_eval_batches = len(df_valid_fold) // batch_size + int(len(df_valid_fold) % batch_size != 0)\n",
    "    num_test_batches = len(test_df) // batch_size + int(len(test_df) % batch_size != 0)\n",
    "    \n",
    "    # initialize test predictions\n",
    "    test_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "    test_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "        optimizer, 'dynamic')\n",
    "\n",
    "    # config = RobertaConfig(output_hidden_states=True, num_labels=2)\n",
    "    # config = RobertaConfig.from_json_file(os.path.join(PATH, \"config.json\"))\n",
    "    # config.output_hidden_states = True\n",
    "    # config.num_labels = 2\n",
    "    # RoBertQAModel.DROPOUT_RATE = 0.2\n",
    "    # RoBertQAModel.NUM_HIDDEN_STATES = 2\n",
    "    # model = RoBertQAModel.from_pretrained(PATH, config=config)\n",
    "    config = BertConfig(output_hidden_states=True, num_labels=2)\n",
    "    BertQAModel.DROPOUT_RATE = 0.2\n",
    "    BertQAModel.NUM_HIDDEN_STATES = 2\n",
    "    model = BertQAModel.from_pretrained(PATH, config=config)\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    loss_step = []\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "    train_dataset = TweetSentimentDataset.create(\n",
    "        df_train_fold, batch_size, shuffle_buffer_size=2048)\n",
    "    valid_dataset = TweetSentimentDataset.create(\n",
    "        df_valid_fold, batch_size, shuffle_buffer_size=-1)\n",
    "    test_dataset = TweetSentimentDataset.create(\n",
    "        test_df, batch_size, shuffle_buffer_size=-1)\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    for epoch_num in range(num_epochs):\n",
    "        # train for an epoch\n",
    "        train(model, train_dataset, loss_fn, optimizer, global_step, loss_step, num_train_batches, fold)\n",
    "\n",
    "        # predict validation set and compute jaccardian distances\n",
    "        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "            predict(model, valid_dataset, loss_fn, optimizer, num_eval_batches, fold)\n",
    "\n",
    "        selected_text_pred = decode_prediction(\n",
    "            pred_start, pred_end, text, offset, sentiment)\n",
    "        jaccards = []\n",
    "        for i in range(len(selected_text)):\n",
    "            jaccards.append(\n",
    "                jaccard(selected_text[i], selected_text_pred[i]))\n",
    "\n",
    "        score = np.mean(jaccards)\n",
    "\n",
    "        plt.plot(list(range(global_step.numpy())), loss_step)\n",
    "        plt.show()\n",
    "        print(\"fold = %d , epoch = %d , jaccard = %f\" % (fold, epoch_num+1, score))\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            # requires you to have 'fold-{fold_num}' folder in PATH:\n",
    "            # model.save_pretrained(PATH+f'fold-{fold_num}')\n",
    "            # or\n",
    "            # model.save_weights(PATH + f'fold-{fold_num}.h5')\n",
    "\n",
    "            # predict test set\n",
    "            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n",
    "                predict(model, test_dataset, loss_fn, optimizer, num_test_batches, fold)\n",
    "\n",
    "    # add epoch's best test preds to test preds arrays\n",
    "    test_preds_start += test_pred_start\n",
    "    test_preds_end += test_pred_end\n",
    "\n",
    "    # reset model, as well as session and graph (to avoid OOM issues?) \n",
    "    session = tf.compat.v1.get_default_session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    del session, graph, model\n",
    "    model = BertQAModel.from_pretrained(PATH, config=config)\n",
    "    return (test_preds_start, test_preds_end, test_text, test_sentiment, test_offset)\n",
    "    \n",
    "test_result = Parallel(n_jobs=num_folds, backend=\"threading\", verbose=10)(delayed(run)(i) for i in range(num_folds))\n",
    "\n",
    "# initialize test predictions\n",
    "test_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "test_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "\n",
    "for r in test_result:\n",
    "    test_preds_start += r[0]\n",
    "    test_preds_end += r[1]\n",
    "\n",
    "# decode test set and add to submission file\n",
    "test_text = r[2]\n",
    "test_offset = r[4]\n",
    "test_sentiment = r[3]\n",
    "selected_text_pred = decode_prediction(\n",
    "    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n",
    "\n",
    "submission_df.loc[:, 'selected_text'] = selected_text_pred\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}