{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, I analyze and visualize the outliers of the NLP solution from very good notebook \"[TSE2020] RoBERTa (CNN) & Random Seed Distribution\"(https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution) using the functions from my notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert) including PCA processing, Kmeans clustering, WordCloud and others. More over I try to improve the original solution.\n",
    "\n",
    "Add chapters \"**Subtext analysis**\" and \"**Metric analysis**\" from the commit 10."
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results of analysis:\n",
    "1. Outlier analysis of the best solutions on basic roBERTa - pls. see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/155419\n",
    "2. Analysis of the predictions with the worst score=0 from roBERTa - pls. see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/155616\n",
    "3. New (commit 22): **analysis of 3 or more repetitions of characters in words**"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Acknowledgements\n",
    "* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)\n",
    "* [COVID-19 (Week5) Global Forecasting - EDA&ExtraTR](https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr)\n",
    "* [TSE2020] RoBERTa (CNN) & Random Seed Distribution (https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution)\n",
    "* Chris Deotte's post: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/142404#809872\n",
    "* [Faster (2x) TF roBERTa](https://www.kaggle.com/seesee/faster-2x-tf-roberta)\n",
    "* Many thanks to Chris Deotte for his TF roBERTa dataset at https://www.kaggle.com/cdeotte/tf-roberta\n",
    "* https://www.kaggle.com/abhishek/roberta-inference-5-folds"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Import libraries](#1)\n",
    "1. [Download data & FE](#2)\n",
    "1. [Model tuning](#3)\n",
    "   - [My upgrade of parameters](#3.1)\n",
    "   - [Model training](#3.2)\n",
    "1. [Submission](#4)\n",
    "1. [Outlier analysis](#5)\n",
    "    - [Training prediction result visualization](#5.1)\n",
    "    - [WordCloud](#5.2)\n",
    "    - [Subtext analysis](#5.3)\n",
    "    - [Metric analysis](#5.4)\n",
    "    - [PCA visualization](#5.5)\n",
    "    - [Clustering](#5.6)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from transformers import RobertaConfig, TFRobertaPreTrainedModel, TFRobertaMainLayer, TFRobertaModel\n",
    "from transformers.modeling_tf_roberta import TFRobertaEmbeddings\n",
    "from transformers.tokenization_utils import BatchEncoding\n",
    "from transformers.modeling_tf_utils import shape_list\n",
    "\n",
    "pd.set_option('max_colwidth', 40)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Download data & FE <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 3 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Model tuning <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1. My upgrade of parameters <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "Dropout_new = 0.16    # originally 0.1\n",
    "n_split = 5            # originally 5\n",
    "lr = 3e-5              # originally 3e-5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Previous successful commits"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 3 (with original parameters)\n",
    "\n",
    "* Dropout_new = 0.1\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "\n",
    "LB = 0.711"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 5\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "\n",
    "LB = 0.713"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 6\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 7\n",
    "* lr = 3e-5\n",
    "\n",
    "LB = 0.709"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 7\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 4e-5\n",
    "\n",
    "LB = 0.709"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 8\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 2e-5\n",
    "\n",
    "LB = 0.712"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 9\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "* LeakyReLU_alpha=0.05\n",
    "\n",
    "LB = 0.711"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 10\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "* LeakyReLU_alpha=0.3\n",
    "\n",
    "LB = 0.711"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 12\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "* SEED = 42\n",
    "\n",
    "LB = 0.711"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 13\n",
    "\n",
    "* Dropout_new = 0.16\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "\n",
    "LB = 0.711"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 14\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "\n",
    "**LB = 0.715 (the best)**"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 15\n",
    "\n",
    "* Dropout_new = 0.16\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "* SEED = 777\n",
    "\n",
    "LB = 0.710"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 17\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 1e-5\n",
    "\n",
    "LB = 0.709"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 18\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "* BATCH_SIZE = 24      # originally 32\n",
    "\n",
    "LB = 0.704"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 19\n",
    "\n",
    "* Dropout_new = 0.125\n",
    "* n_split = 5\n",
    "* lr = 3e-5\n",
    "\n",
    "LB = 0.711"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 20\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 1e-4\n",
    "\n",
    "LB = 0.709"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Commit 21\n",
    "\n",
    "* Dropout_new = 0.15\n",
    "* n_split = 5\n",
    "* lr = 1e-4\n",
    "* num_cnn2 = 96          # originally 64\n",
    "\n",
    "LB = 0.712"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2. Model training <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972\n",
    "\n",
    "**Upgrade:** add prediction for training data for Outlier analysis and parameters tuning"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "class TFRobertaMainLayer1(TFBertMainLayer):\n",
    "    \"\"\"\n",
    "    Same as TFBertMainLayer but uses TFRobertaEmbeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        self.embeddings = TFRobertaEmbeddings(config, name=\"embeddings\")\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "            self,\n",
    "            inputs,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
    "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
    "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
    "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
    "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
    "            assert len(inputs) <= 6, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            assert len(inputs) <= 6, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = tf.fill(input_shape, 1)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "\n",
    "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\n",
    "        return embedding_output, extended_attention_mask, head_mask\n",
    "\n",
    "    def call_run(self, embedding_output, extended_attention_mask, head_mask, training):\n",
    "        encoder_outputs = self.encoder([embedding_output, extended_attention_mask, head_mask], training=training)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RoBertQAModel1(TFRobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "\n",
    "        self.robert = TFRobertaMainLayer1(config, name=\"roberta\")\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        embedding_output, extended_attention_mask, head_mask = self.robert(inputs, **kwargs)\n",
    "        x = self.robert.call_run(embedding_output, extended_attention_mask, head_mask, training=kwargs.get(\"training\", False))\n",
    "        \n",
    "        return embedding_output, extended_attention_mask, head_mask, x\n",
    "    \n",
    "    @tf.function\n",
    "    def adversarial(self, x1, extended_attention_mask, head_mask, y_true, loss_fn):\n",
    "        \"\"\"\n",
    "        Adversarial training\n",
    "        \"\"\"\n",
    "        y_pred = self.call_run(x1, extended_attention_mask, head_mask, training=True)\n",
    "        loss  = (loss_fn(y_true[0], y_pred[0]) + loss_fn(y_true[1], y_pred[1])) / 2\n",
    "        perturb = tf.gradients(loss, x1, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "        \n",
    "        #reciprocal in l2 normal\n",
    "        perturb = 0.02 * tf.math.l2_normalize(tf.stop_gradient(perturb), axis=1)\n",
    "        x1 = x1 + perturb        \n",
    "        \n",
    "        # adv_loss\n",
    "        y_pred = self.call_run(x1, extended_attention_mask, head_mask, training=True)\n",
    "        adv_loss  = loss_fn(y_true[0], y_pred[0]) + loss_fn(y_true[1], y_pred[1])\n",
    "        return adv_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def virtual_adversarial(self, x1, extended_attention_mask, head_mask, y_pred, power_iterations=1, p_mult=0.02):\n",
    "        bernoulli = tfp.distributions.Bernoulli\n",
    "        # y_pred = self.call_run(x1, extended_attention_mask, head_mask, training=True)\n",
    "        prob1 = tf.clip_by_value((y_pred[0] + y_pred[1]) / 2, 1e-7, 1. - 1e-7)\n",
    "        prob_dist1 = bernoulli(probs=prob1)\n",
    "        # prob2 = tf.clip_by_value(y_pred[1], 1e-7, 1. - 1e-7)\n",
    "        # prob_dist2 = bernoulli(probs=prob2)\n",
    "\n",
    "        # generate virtual adversarial perturbation\n",
    "        d1 = tf.keras.backend.random_uniform(shape=tf.shape(x1), dtype=tf.dtypes.float32)\n",
    "        # d2 = tf.keras.backend.random_uniform(shape=tf.shape(x1), dtype=tf.dtypes.float32)\n",
    "        for _ in range(power_iterations):\n",
    "            d1 = (0.02) * tf.math.l2_normalize(d1, axis=1)\n",
    "            # d2 = (0.02) * tf.math.l2_normalize(d2, axis=1)\n",
    "            y_pred1 = self.call_run(x1 + d1, extended_attention_mask, head_mask, training=True)\n",
    "            # y_pred2 = self.call_run(x1 + d2 , extended_attention_mask, head_mask, training=True)\n",
    "            p_prob1 = tf.clip_by_value((y_pred1[0] + y_pred1[1]) / 2, 1e-7, 1. - 1e-7)\n",
    "            # p_prob2 = tf.clip_by_value(y_pred1[1], 1e-7, 1. - 1e-7)\n",
    "            kl1 = tfp.distributions.kl_divergence(prob_dist1, bernoulli(probs=p_prob1), allow_nan_stats=False)\n",
    "            # kl2 = tfp.distributions.kl_divergence(prob_dist2, bernoulli(probs=p_prob2), allow_nan_stats=False)\n",
    "\n",
    "            gradient1 = tf.gradients(kl1, [d1], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "            # gradient2 = tf.gradients(kl2, [d2], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n",
    "            d1 = tf.stop_gradient(gradient1)\n",
    "            # d2 = tf.stop_gradient(gradient2)\n",
    "        d1 = p_mult * tf.math.l2_normalize(d1, axis=1)\n",
    "        # d2 = p_mult * tf.math.l2_normalize(d2, axis=1)\n",
    "        tf.stop_gradient(prob1)\n",
    "        # tf.stop_gradient(prob2)\n",
    "\n",
    "        # virtual adversarial loss\n",
    "        y_pred1 = self.call_run(x1 + d1, extended_attention_mask, head_mask, training=True)\n",
    "        # y_pred2 = self.call_run(x1 + d2 , extended_attention_mask, head_mask, training=True)\n",
    "        p_prob1 = tf.clip_by_value((y_pred1[0] + y_pred1[1]) / 2, 1e-7, 1. - 1e-7)\n",
    "        # p_prob2 = tf.clip_by_value(y_pred1[1], 1e-7, 1. - 1e-7)\n",
    "        v_adv_loss1 = tfp.distributions.kl_divergence(prob_dist1, bernoulli(probs=p_prob1), allow_nan_stats=False)\n",
    "        # v_adv_loss2 = tfp.distributions.kl_divergence(prob_dist2, bernoulli(probs=p_prob2), allow_nan_stats=False)\n",
    "        return tf.reduce_mean(v_adv_loss1)\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = RoBertQAModel1.from_pretrained(PATH+'pretrained-roberta-base.h5', config=config)\n",
    "    embedding_output, extended_attention_mask, head_mask, x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_kg_hide-input": true
   },
   "cell_type": "code",
   "source": "ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "%%time\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting all Train for Outlier analysis...')\n    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n    preds_start_train += preds_train[0]/skf.n_splits\n    preds_end_train += preds_train[1]/skf.n_splits\n\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\nprint(jac) # Jaccard CVs",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Submission <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true,
    "_kg_hide-input": true
   },
   "cell_type": "code",
   "source": "all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\ntest.sample(10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Outlier analysis <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.1. Training prediction result visualization <a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Visualization training prediction results\nall = []\nstart = []\nend = []\nstart_pred = []\nend_pred = []\nfor k in range(input_ids.shape[0]):\n    a = np.argmax(preds_start_train[k,])\n    b = np.argmax(preds_end_train[k,])\n    start.append(np.argmax(start_tokens[k]))\n    end.append(np.argmax(end_tokens[k]))        \n    if a>b:\n        st = train.loc[k,'text']\n        start_pred.append(0)\n        end_pred.append(len(st))\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n        start_pred.append(a)\n        end_pred.append(b)\n    all.append(st)\ntrain['start'] = start\ntrain['end'] = end\ntrain['start_pred'] = start_pred\ntrain['end_pred'] = end_pred\ntrain['selected_text_pred'] = all\ntrain.sample(10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def metric_tse(df,col1,col2):\n    # Calc metric of tse-competition - according to https://www.kaggle.com/c/tweet-sentiment-extraction/overview/evaluation\n    return df.apply(lambda x: jaccard(x[col1],x[col2]),axis=1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Analytics\ntrain = train.replace({'sentiment': {'negative': -1, 'neutral': 0, 'positive': 1}})\ntrain['len_text'] = train['text'].str.len()\ntrain['len_selected_text'] = train['selected_text'].str.len()\ntrain['diff_num'] = train['end']-train['start']\ntrain['share'] = train['len_selected_text']/train['len_text']",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Prediction analytics\ntrain['selected_text_pred'] = train['selected_text_pred'].map(lambda x: x.lstrip(' '))\ntrain['len_selected_text_pred'] = train['selected_text_pred'].str.len()\ntrain['diff_num_pred'] = train['end_pred']-train['start_pred']\ntrain['share_pred'] = train['len_selected_text_pred']/train['len_text']\n# len_equal\ntrain['len_equal'] = 0\ntrain.loc[(train['start'] == train['start_pred']) & (train['end'] == train['end_pred']), 'len_equal'] = 1\n# metric\ntrain['metric'] = metric_tse(train,'selected_text','selected_text_pred')\n# res\ntrain['res'] = 0\ntrain.loc[train['metric'] == 1, 'res'] = 1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def rep_3chr(text):\n    # Checks if there are 3 or more repetitions of characters in words\n    chr3 = 0\n    for word in text.split():\n        for c in set(word):\n            if word.rfind(c+c+c) > -1:\n                chr3 = 1                \n    return chr3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Analysis of 3 or more repetitions of characters in words\ntrain['text_chr3'] = train['text'].apply(rep_3chr)\ntrain['selected_text_chr3'] = train['selected_text'].apply(rep_3chr)\ntrain['selected_text_pred_chr3'] = train['selected_text_pred'].apply(rep_3chr)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# result\ncol_interesting = ['sentiment', 'len_text', 'text_chr3', 'selected_text', 'len_selected_text', 'diff_num', 'share', \n                   'selected_text_chr3', 'selected_text_pred', 'len_selected_text_pred', 'diff_num_pred', 'share_pred',\n                   'selected_text_pred_chr3', 'len_equal', 'metric', 'res']\ntrain[col_interesting].head(10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Total metric =',train['metric'].mean())",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Long 'selected text' are not predicted correctly (get too long)",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Outlier\ntrain_outlier = train[train['res'] == 0].reset_index(drop=True)\ntrain_outlier",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "sh_out = str(round(len(train_outlier)*100/len(train),1))\nprint('Number of outliers is ' + sh_out + '% from training data')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Good prediction\ntrain_good = train[train['res'] == 1].reset_index(drop=True)\ntrain_good",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_good.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Share of all data')\ntrain[['share', 'share_pred']].hist(bins=10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Share of outlier data')\ntrain_outlier[['share', 'share_pred']].hist(bins=10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "markdown",
   "source": "The main problem is in the predicting of the longest and shortest selected_text which are most or least different from the given text",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Only one word in 'selected_text'\ntrain_outlier[train_outlier['diff_num']==0]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_good[train_good['diff_num']==0]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# 'selected_text' = 'text'\ntrain_outlier[train_outlier['share']==1]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_good[train_good['share']==1]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Only one word in 'text'\ntrain_outlier[train_outlier[\"text\"].str.find(' ') == -1].head(5)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "len(train_outlier[train_outlier[\"text\"].str.find(' ') == -1])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_good[train_good[\"text\"].str.find(' ') == -1].head(5)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "len(train_good[train_good[\"text\"].str.find(' ') == -1])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Text from a single word almost always processes correctly",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.2. WordCloud <a class=\"anchor\" id=\"5.2\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using my notebook https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert",
   "execution_count": null
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "def plot_word_cloud(x, col):\n    corpus=[]\n    for k in x[col].str.split():\n        for i in k:\n            corpus.append(i)\n    plt.figure(figsize=(12,8))\n    word_cloud = WordCloud(\n                              background_color='black',\n                              max_font_size = 80\n                             ).generate(\" \".join(corpus[:50]))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()\n    return corpus[:50]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# All training text\nprint('Word Cloud for all text in training data')\ntrain_all = plot_word_cloud(train, 'text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_all",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# All test\nprint('Word Cloud for all text in test')\ntest_all = plot_word_cloud(test, 'text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "test_all",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# All training selected_text\nprint('Word Cloud for selected_text in training data')\ntrain_selected_text = plot_word_cloud(train, 'selected_text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_selected_text",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Oitlier WordCloud\nprint('Word Cloud for Outliers')\noutlier_max = plot_word_cloud(train_outlier, 'selected_text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "outlier_max",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Worst oitlier WordCloud\nprint('Word Cloud for the 100 worst outliers')\noutlier_max100 = plot_word_cloud(train_outlier.nsmallest(100, 'metric', keep='all'), 'selected_text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "outlier_max100",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Worst oitlier WordCloud\nprint('Word Cloud for the 1000 worst outliers')\noutlier_max1000 = plot_word_cloud(train_outlier.nsmallest(1000, 'metric', keep='all'), 'selected_text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "outlier_max1000",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Good prediction WordCloud\nprint('Word Cloud for good prediction')\ngood_max = plot_word_cloud(train_good, 'selected_text')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "good_max",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.3. Subtext analysis <a class=\"anchor\" id=\"5.3\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "def subtext_analysis(col, subtext, df1, str1, df2, str2):\n    # Calc statistics as table for subtext in the df1[col] (smaller) in compare to df2[col] (bigger) \n    \n    result = pd.DataFrame(columns = ['subtext', str1, str2, 'share,%'])\n    if (len(df1) > 0) and (len(df2) > 0):\n        for i in range(len(subtext)):\n            result.loc[i,'subtext'] = subtext[i]\n            num1 = len(df1[df1[col].str.find(subtext[i]) > -1])\n            result.loc[i, str1] = num1\n            num2 = len(df2[df2[col].str.find(subtext[i]) > -1])\n            result.loc[i, str2] = num2\n            result.loc[i,'share,%'] = round(num1*100/num2,1) if num2 != 0 else 0\n    print('Number of all data is', len(df2))\n    display(result.sort_values(by=['share,%', str1], ascending=False))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def subtext_analysis_one_df(col, subtext, df, str):\n    # Calc statistics as table for subtext in the df[col]\n    \n    result = pd.DataFrame(columns = ['subtext', str, 'share of all,%'])\n    num_all = len(df)\n    if (num_all > 0):\n        for i in range(len(subtext)):\n            result.loc[i,'subtext'] = subtext[i]\n            num = len(df[df[col].str.find(subtext[i]) > -1])\n            result.loc[i, str] = num\n            result.loc[i,'share of all,%'] = round(num*100/num_all,1)\n    print('Number of all data is', len(df))\n    display(result.sort_values(by='share of all,%', ascending=False))    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_test = ['SAD', 'bullying', 'Uh', 'oh', 'onna', 'fun', 'addicted', 'Power', 'well', 'unhappy', 'funny', 'Tears', 'Fears', 'sleeeeepy', ' ', ',', '?', '!' ,'!!', '!!!', ':/', '...', 'http', '****']",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_outlier, 'train_outliers', train, 'train_all')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are problems in processing: \"!\", \"!!\", \"!!!\", \":/\", \"...\", \"http\" etc.",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_good, 'train_good', train, 'train_all')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis_one_df(\"selected_text\", subtext_test, train, 'test_all')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis_one_df(\"selected_text\", subtext_test, test, 'test_all')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "test['text_chr3'] = test['text'].apply(rep_3chr)\ntest.head(10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "test.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.4. Metric analysis <a class=\"anchor\" id=\"5.4\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Metric of prediction for training data')\ntrain[['metric']].hist(bins=10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Metric of prediction for outliers of training data')\ntrain_outlier[['metric']].hist(bins=10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier1 = train_outlier.nsmallest(1000, 'metric', keep='all')\ntrain_outlier2 = train_outlier.nsmallest(2000, 'metric', keep='all')\ntrain_outlier3 = train_outlier.nsmallest(3000, 'metric', keep='all')\ntrain_outlier5 = train_outlier.nsmallest(5000, 'metric', keep='all')\ntrain_outlier8 = train_outlier.nsmallest(8000, 'metric', keep='all')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_outlier1, 'in worst 1000 outliers', train_outlier, 'in all outliers')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_outlier2, 'in worst 2000 outliers', train_outlier, 'in all outliers')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_outlier3, 'in worst 3000 outliers', train_outlier, 'in all outliers')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_outlier5, 'in worst 5000 outliers', train_outlier, 'in all outliers')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "subtext_analysis(\"selected_text\", subtext_test, train_outlier8, 'in worst 8000 outliers', train_outlier, 'in all outliers')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier1.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier2.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier3.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier5.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_outlier8.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Histograms of interesting features in training data\ncol_hist = ['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', \n            'text_chr3', 'selected_text_chr3', 'selected_text_pred_chr3', 'metric']",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Statistics for 1000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier1[col_hist].hist(ax=ax)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Statistics for 2000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier2[col_hist].hist(ax=ax)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Statistics for 3000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier3[col_hist].hist(ax=ax)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Statistics for 5000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier5[col_hist].hist(ax=ax)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print('Statistics for 8000 worst outliers')\nfig = plt.figure(figsize = (8,8))\nax = fig.gca()\ntrain_outlier8[col_hist].hist(ax=ax)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.5. PCA visualization <a class=\"anchor\" id=\"5.5\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using my notebook https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True, title=None):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.title(title)\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Good')\n            blue_patch = mpatches.Patch(color='blue', label='Outlier')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "fig = plt.figure(figsize=(16, 16))          \nplot_LSA(preds_start_train, train['res'], title='Predicted start places of selected text in training data')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "fig = plt.figure(figsize=(16, 16))          \nplot_LSA(preds_end_train, train['res'], title='Predicted end places of selected text in training data')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are a number of clear patterns that allow us to hope that we can improve the solution.",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.6. Clustering <a class=\"anchor\" id=\"5.6\"></a>\n\n[Back to Table of Contents](#0.1)",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using my notebook https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr",
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "data = train[['sentiment', 'start', 'end', 'start_pred', 'end_pred', 'len_text', 'len_selected_text', 'diff_num', 'share', 'metric', 'res']].dropna()\ndata",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Thanks to https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering\ninertia = []\npca = PCA(n_components=2)\n# fit X and apply the reduction to X \nx_3d = pca.fit_transform(data)\nfor k in range(1, 8):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(x_3d)\n    inertia.append(np.sqrt(kmeans.inertia_))\nplt.plot(range(1, 8), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Thanks to https://www.kaggle.com/arthurtok/a-cluster-of-colors-principal-component-analysis\n# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=5, random_state=0)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_3d)\nLABEL_COLOR_MAP = {0 : 'r',\n                   1 : 'g',\n                   2 : 'b',\n                   3 : 'y',\n                   4 : 'c'}\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (7,7))\nplt.scatter(x_3d[:,0],x_3d[:,1], c= label_color, alpha=0.9)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are a number of clear clusters that allow us to hope that we can improve the solution.",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I hope you find this kernel useful and enjoyable.",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Your comments and feedback are most welcome.",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[Go to Top](#0)",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}