{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Complete TensorFlow mixed-precision implementation with Bert\n",
    "\n",
    "*1.use bert in tensorflow 2.1*\n",
    "*2.add the sentiment frquence: positive negative neutral*\n",
    "*3.use lr warmup*\n",
    "*4. focal loss*\n",
    "*5.get the best logits*\n",
    "*6.data argumentation*"
   ]
  },
  {
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn import model_selection\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from transformers import RobertaConfig, TFRobertaPreTrainedModel, TFRobertaMainLayer, TFRobertaModel\n",
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "stop"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# read csv files\n",
    "train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df.loc[:, \"selected_text\"] = test_df.text.values\n",
    "\n",
    "submission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "\n",
    "print(\"train shape =\", train_df.shape)\n",
    "print(\"test shape  =\", test_df.shape)\n",
    "\n",
    "# set some global variables\n",
    "PATH = \"../input/tf-roberta/\"\n",
    "MAX_SEQUENCE_LENGTH = 96\n",
    "# TOKENIZER = BertWordPieceTokenizer(f\"../input/bert-base-uncased/vocab.txt\", lowercase=True, add_special_tokens=False)\n",
    "TOKENIZER = ByteLevelBPETokenizer(vocab_file=f\"{PATH}/vocab-roberta-base.json\", \n",
    "                                  merges_file=f\"{PATH}/merges-roberta-base.txt\", \n",
    "                                  lowercase=True, \n",
    "                                  add_prefix_space=True)\n",
    "\n",
    "sentiment_dict = {\"positive\": [\"good\", \"happy\", \"love\", \"day\", \"thanks\", \"great\", \"fun\", \"nice\", \"hope\", \"thank\"],\n",
    "                  \"negative\": [\"miss\", \"sad\", \"sorry\", \"bad\", \"hate\", \"sucks\", \"sick\", \"like\", \"feel\", \"bored\"],\n",
    "                  \"neutral\": [\"get\", \"go\", \"day\", \"work\", \"going\", \"quot\", \"lol\", \"got\", \"like\", \"today\"]}\n",
    "\n",
    "# let's take a look at the data\n",
    "train_df.head(10)\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": "train shape = (27480, 4)\ntest shape  = (3534, 4)\n",
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "execution_count": 2,
     "data": {
      "text/plain": "       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n7  50e14c0bb8                                         Soooo high   \n8  e050245fbd                                        Both of you   \n9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n\n                                       selected_text sentiment  \n0                I`d have responded, if I were going   neutral  \n1                                           sooo sad  negative  \n2                                        bullying me  negative  \n3                                     leave me alone  negative  \n4                                  sons of * * * * ,  negative  \n5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n6                                                fun  positive  \n7                                         Soooo high   neutral  \n8                                        Both of you   neutral  \n9                   wow . . . u just became cooler .  positive  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>sooo sad</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>sons of * * * * ,</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>28b57f3990</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6e0c6d75b1</td>\n      <td>2am feedings for the baby are fun when he is a...</td>\n      <td>fun</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>50e14c0bb8</td>\n      <td>Soooo high</td>\n      <td>Soooo high</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>e050245fbd</td>\n      <td>Both of you</td>\n      <td>Both of you</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fc2cbefa9d</td>\n      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n      <td>wow . . . u just became cooler .</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "I. Set up preprocessing and dataset/datagenerator\n",
    "```\n"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def preprocess(tweet, selected_text, sentiment):\n",
    "    \"\"\"\n",
    "    Will be used in tf.data.Dataset.from_generator(...)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # The original strings have been converted to\n",
    "    # byte strings, so we need to decode it\n",
    "    tweet = tweet.decode('utf-8')\n",
    "    selected_text = selected_text.decode('utf-8')\n",
    "    sentiment = sentiment.decode('utf-8')\n",
    "\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    # tokenize with offsets\n",
    "    enc = TOKENIZER.encode(tweet)\n",
    "    input_ids_orig = enc.ids\n",
    "    offsets = enc.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    target_start = target_idx[0]\n",
    "    target_end = target_idx[-1]\n",
    "    \n",
    "    # add sentiment word frequency\n",
    "    sentiment_frequency = []\n",
    "    pos_fre = 0\n",
    "    neg_fre = 0\n",
    "    neu_fre = 0\n",
    "    for token in enc.tokens:\n",
    "        token = token.replace(\"Ġ\", \"\")\n",
    "        if token in sentiment_dict[\"positive\"]:\n",
    "            pos_fre += 1\n",
    "        if token in sentiment_dict[\"negative\"]:\n",
    "            neg_fre += 1\n",
    "        if token in sentiment_dict[\"neutral\"]:\n",
    "            neu_fre += 1\n",
    "    sentiment_frequency.append(str(pos_fre))\n",
    "    sentiment_frequency.append(str(neg_fre))\n",
    "    sentiment_frequency.append(str(neu_fre))\n",
    "    enc_sentiment = TOKENIZER.encode(\" \".join(sentiment_frequency))\n",
    "    \n",
    "    \n",
    "    # add and pad data (hardcoded for BERT)\n",
    "    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n",
    "    sentiment_map = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "\n",
    "    input_ids = [0] + input_ids_orig + [2] + [2] + [sentiment_map[sentiment]] + enc_sentiment.ids + [2]\n",
    "    input_type_ids = [0] * 1 + [0] * (len(input_ids_orig) + 7)\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 8)\n",
    "    offsets = [(0, 0)] + offsets + [(0, 0)] * 7\n",
    "    target_start += pos_offsets\n",
    "    target_end += pos_offsets\n",
    "\n",
    "    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        input_type_ids = input_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "    elif padding_length < 0:\n",
    "        input_ids = input_ids[:padding_length - 1] + [2]\n",
    "        attention_mask = attention_mask[:padding_length - 1] + [1]\n",
    "        input_type_ids = input_type_ids[:padding_length - 1] + [0]\n",
    "        offsets = offsets[:padding_length - 1] + [(0, 0)]\n",
    "        if target_start >= MAX_SEQUENCE_LENGTH:\n",
    "            target_start = MAX_SEQUENCE_LENGTH - 1\n",
    "        if target_end >= MAX_SEQUENCE_LENGTH:\n",
    "            target_end = MAX_SEQUENCE_LENGTH - 1\n",
    "\n",
    "    return (\n",
    "        input_ids, attention_mask, input_type_ids, offsets,\n",
    "        target_start, target_end, tweet, selected_text, sentiment,\n",
    "    )\n",
    "\n",
    "class TweetSentimentDataset(tf.data.Dataset):\n",
    "    \n",
    "    OUTPUT_TYPES = (\n",
    "        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n",
    "        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n",
    "        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_SHAPES = (\n",
    "        (MAX_SEQUENCE_LENGTH,),   (MAX_SEQUENCE_LENGTH,), (MAX_SEQUENCE_LENGTH,), \n",
    "        (MAX_SEQUENCE_LENGTH, 2), (),                     (),\n",
    "        (),                       (),                     (),\n",
    "    )\n",
    "    \n",
    "    # AutoGraph will automatically convert Python code to\n",
    "    # Tensorflow graph code. You could also wrap 'preprocess' \n",
    "    # in tf.py_function(..) for arbitrary python code\n",
    "    def _generator(tweet, selected_text, sentiment):\n",
    "        for tw, st, se in zip(tweet, selected_text, sentiment):\n",
    "            yield preprocess(tw, st, se)\n",
    "    \n",
    "    # This dataset object will return a generator\n",
    "    def __new__(cls, tweet, selected_text, sentiment):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=cls.OUTPUT_TYPES,\n",
    "            output_shapes=cls.OUTPUT_SHAPES,\n",
    "            args=(tweet, selected_text, sentiment)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n",
    "        dataset = TweetSentimentDataset(\n",
    "            dataframe.text.values, \n",
    "            dataframe.selected_text.values, \n",
    "            dataframe.sentiment.values\n",
    "        )\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle_buffer_size != -1:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        # d = next(iter(dataset))\n",
    "        # print(\"Writing example in %d\" % (len(dataframe)))\n",
    "        # for i in range(5):\n",
    "        #     print(\"*** Example ***\")\n",
    "        #     print(\"tokens: %s\" % \" \".join(TOKENIZER.encode(d[6].numpy()[i].decode(\"utf-8\")).tokens))\n",
    "        #     print(\"input_ids: %s\" % \" \".join([str(x) for x in d[0].numpy()[i]]))\n",
    "        #     print(\"input_mask: %s\" % \" \".join([str(x) for x in d[1].numpy()[i]]))\n",
    "        #     print(\"segment_ids: %s\" % \" \".join([str(x) for x in d[2].numpy()[i]]))\n",
    "        #     print(\"selected_text: %s\" % d[7].numpy()[i].decode(\"utf-8\"))\n",
    "        #     print(\"idx_start: %d\" % d[4].numpy()[i])\n",
    "        #     print(\"idx_end: %d\" % d[5].numpy()[i])\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "def generate_fold_data(data, num_folds):\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold_num, (train_idx, valid_idx) in enumerate(kfold.split(X=data.text, y=data.sentiment.values)):\n",
    "        if fold_num == 0:\n",
    "            save_data = data.iloc[valid_idx]\n",
    "            save_data[\"kfold\"] = fold_num\n",
    "        else:\n",
    "            _save_data = data.iloc[valid_idx]\n",
    "            _save_data[\"kfold\"] = fold_num\n",
    "            save_data = pd.concat([save_data, _save_data], axis=0)\n",
    "            \n",
    "    save_data = save_data.reset_index(drop=True)\n",
    "    # print(save_data.shape)\n",
    "    # save_data.to_csv(\"train_5folds.csv\", index=False)\n",
    "    return save_data"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_special_char(text_orign, selected_text_orign):\n",
    "    tweet = \" \" + \" \".join(str(text_orign).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text_orign).split())\n",
    "    \n",
    "    if \"¿½\" in tweet and \"¿½\" not in selected_text:\n",
    "        tweet = tweet.replace(\"¿½\", \"\")\n",
    "        \n",
    "    if \"ï\" in tweet and \"ï\" not in selected_text:\n",
    "        tweet = tweet.replace(\"ï\", \"i\")\n",
    "        \n",
    "    return tweet.strip()\n",
    "    \n",
    "def align_texts_in_roberta(text_orign, selected_text_orign, tokenizer):\n",
    "    \"\"\"\n",
    "    align the text_orign and selected_text_orign\n",
    "    :param text_orign:\n",
    "    :param selected_text_orign:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tweet = \" \" + \" \".join(str(text_orign).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text_orign).split())\n",
    "\n",
    "    text_list = tokenizer.encode(tweet)\n",
    "    offsets = text_list.offsets\n",
    "\n",
    "    selected_text_list = tokenizer.encode(selected_text)\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    target_start = target_idx[0]\n",
    "    target_end = target_idx[-1]\n",
    "\n",
    "    decoded_text = \"\"\n",
    "    for i in range(target_start, target_end+1):\n",
    "        decoded_text += tweet[offsets[i][0]:offsets[i][1]]\n",
    "        if (i+1) < len(offsets) and offsets[i][1] < offsets[i+1][0]:\n",
    "            decoded_text += \" \"\n",
    "    return decoded_text.strip()\n",
    "\n",
    "train_df[\"text\"] = train_df.apply(lambda x: replace_special_char(x[\"text\"], x[\"selected_text\"]), axis=1)\n",
    "train_df[\"selected_text\"] = train_df.apply(lambda x : align_texts_in_roberta(x[\"text\"], x[\"selected_text\"], TOKENIZER), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "        if text is not None:\n",
    "            result = []\n",
    "            itokens_list = text.split(\" \")\n",
    "            for itoken in itokens_list:\n",
    "                if itoken not in stop:\n",
    "                    result.append(itoken)\n",
    "            return \" \".join(result)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    s = ''.join(ch for ch in text if ch not in exclude)\n",
    "    return s\n",
    "\n",
    "def remove_char(text):\n",
    "    del_dict = {\"Aam dogs going die somebody doesnt save them\": \"dogs going die somebody doesnt save them\",\n",
    "                \"HOPE h\": \" HOPE\",\n",
    "                \"excited ab\": \"excited\",\n",
    "                \"released stinks\": \"stinks\",\n",
    "                \"Wave looks interesting http\": \"Wave looks interesting\"}\n",
    "    for key in del_dict:\n",
    "        if key == text:\n",
    "            return del_dict[key]\n",
    "    return text\n",
    "\n",
    "def clean_df(df, train=True):\n",
    "    df[\"dirty_text\"] = df['text']\n",
    "    df['text'] = df['text'].apply(lambda x: remove_emoji(x))\n",
    "    df['text'] = df['text'].apply(lambda x : remove_URL(x))\n",
    "    df['text'] = df['text'].apply(lambda x : remove_html(x))\n",
    "    df['text'] = df['text'].apply(lambda x : remove_stopwords(x)) \n",
    "    df['text'] = df['text'].apply(lambda x : remove_punct(x))\n",
    "    df.text = df.text.replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "    if train:\n",
    "        df['selected_text'] = df['selected_text'].apply(lambda x: remove_emoji(x))\n",
    "        df['selected_text'] = df['selected_text'].apply(lambda x : remove_URL(x))\n",
    "        df['selected_text'] = df['selected_text'].apply(lambda x : remove_html(x))\n",
    "        df['selected_text'] = df['selected_text'].apply(lambda x : remove_stopwords(x))\n",
    "        df['selected_text'] = df['selected_text'].apply(lambda x : remove_punct(x))\n",
    "        df.selected_text = df.selected_text.replace('\\s+', ' ', regex=True)\n",
    "        df['selected_text'] = df['selected_text'].apply(lambda x : remove_char(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def post_process(s):\n",
    "    a = re.findall('[^A-Za-z0-9]',s)\n",
    "    b = re.sub('[^A-Za-z0-9]+', '', s)\n",
    "\n",
    "    try:\n",
    "        if a.count('.')==3:\n",
    "            text = b + '. ' + b + '..'\n",
    "        elif a.count('!')==4:\n",
    "            text = b + '! ' + b + '!! ' +  b + '!!!'\n",
    "        else:\n",
    "            text = s\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# data augmentation \n",
    "train_df_aug = deepcopy(train_df)\n",
    "train_df_aug = clean_df(train_df_aug)\n",
    "train_df_aug = train_df_aug.drop(train_df_aug[train_df_aug[\"dirty_text\"] == train_df_aug[\"text\"]].index).reset_index(drop=True)\n",
    "train_df_aug = train_df_aug.drop(train_df_aug[train_df_aug[\"sentiment\"] == \"neutral\"].index).reset_index(drop=True)\n",
    "train_df_aug.rename(columns={'dirty_text':'kfold'}, inplace=True)\n",
    "train_df_aug = train_df_aug.drop(train_df_aug[train_df_aug[\"selected_text\"] == \"\"].index).reset_index(drop=True)\n",
    "train_df_aug = train_df_aug.drop(train_df_aug[train_df_aug[\"selected_text\"] == \" \"].index).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "```\n",
    "II. Set up transformer model and functions\n",
    "```"
   ]
  },
  {
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class RoBertQAModel(TFRobertaPreTrainedModel):\n",
    "# class RoBertQAModel(TFBertPreTrainedModel):\n",
    "    \n",
    "    DROPOUT_RATE = 0.1\n",
    "    NUM_HIDDEN_STATES = 2\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "        self.bert = TFRobertaModel.from_pretrained(PATH +'pretrained-roberta-base.h5',config=config)\n",
    "        # self.bert = TFRobertaMainLayer(config, name=\"bert\")\n",
    "        self.concat = L.Concatenate()\n",
    "        self.dropout = L.Dropout(self.DROPOUT_RATE)\n",
    "        self.qa_outputs = L.Dense(\n",
    "            config.num_labels, \n",
    "            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "            dtype='float32',\n",
    "            name=\"qa_outputs\")\n",
    "        self.conv1d_128 = L.Conv1D(128, 2, padding='same')\n",
    "        self.conv1d_64 = L.Conv1D(64, 2, padding='same')\n",
    "        self.leakyreLU = L.LeakyReLU()\n",
    "        self.dense = L.Dense(1, dtype='float32')\n",
    "        self.flatten = L.Flatten()\n",
    "        \n",
    "        self.dropout_2 = L.Dropout(self.DROPOUT_RATE)\n",
    "        self.conv1d_128_2 = L.Conv1D(128, 2, padding='same')\n",
    "        self.conv1d_64_2 = L.Conv1D(64, 2, padding='same')\n",
    "        self.leakyreLU_2 = L.LeakyReLU()\n",
    "        self.dense_2 = L.Dense(1, dtype='float32')\n",
    "        self.flatten_2 = L.Flatten()\n",
    "        \n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # outputs: Tuple[sequence, pooled, hidden_states]\n",
    "        x, _, hidden_states = self.bert(inputs, **kwargs)\n",
    "        \n",
    "        x1 = self.dropout(hidden_states[-1], training=kwargs.get(\"training\", False))\n",
    "        x1 = self.conv1d_128(x1)\n",
    "        x1 = self.leakyreLU(x1)\n",
    "        x1 = self.conv1d_64(x1)\n",
    "        x1 = self.dense(x1)\n",
    "        start_logits = self.flatten(x1)\n",
    "        start_logits = tf.keras.layers.Activation('softmax')(start_logits)\n",
    "        \n",
    "        x2 = self.dropout_2(hidden_states[-2], training=kwargs.get(\"training\", False))\n",
    "        x2 = self.conv1d_128_2(x2)\n",
    "        x2 = self.leakyreLU_2(x2)\n",
    "        x2 = self.conv1d_64_2(x2)\n",
    "        x2 = self.dense_2(x2)\n",
    "        end_logits = self.flatten_2(x2)\n",
    "        end_logits = tf.keras.layers.Activation('softmax')(end_logits)\n",
    "    \n",
    "        return start_logits, end_logits\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, y_true, current_step):\n",
    "    def _train_step(inputs, y_true, current_step):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(inputs, training=True)\n",
    "            loss  = loss_fn(y_true[0], y_pred[0])\n",
    "            loss += loss_fn(y_true[1], y_pred[1])\n",
    "            # scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        optimizer.learning_rate = lr_decay_fn(learning_rate, num_train_steps, num_warmup_steps, current_step)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss, y_pred\n",
    "    \n",
    "    loss, y_pred = strategy.experimental_run_v2(_train_step, args=(inputs, y_true, current_step,))\n",
    "    sum_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=0)\n",
    "    return sum_loss, y_pred\n",
    "\n",
    "\"\"\"\n",
    "def train(model, strategy, dataset, loss_fn, optimizer, current_step, loss_step, data_len, fold_num, lr_decay_fn):\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(inputs, y_true, current_step):\n",
    "        def _train_step(inputs, y_true, current_step):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(inputs, training=True)\n",
    "                loss  = loss_fn(y_true[0], y_pred[0])\n",
    "                loss += loss_fn(y_true[1], y_pred[1])\n",
    "                # scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            # gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "            optimizer.learning_rate = lr_decay_fn(learning_rate, num_train_steps, num_warmup_steps, current_step)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss, y_pred\n",
    "        \n",
    "        loss, y_pred = strategy.experimental_run_v2(_train_step, args=(inputs, y_true, current_step,))\n",
    "        sum_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=0)\n",
    "        return sum_loss, y_pred\n",
    "    \n",
    "    epoch_loss = 0.\n",
    "    tk0 = tqdm(dataset, total=data_len, desc=\"Training: \" + str(fold_num))\n",
    "        \n",
    "    with strategy.scope():\n",
    "        for batch_num, sample in enumerate(tk0):\n",
    "            # current_step.assign_add(1)\n",
    "            # loss, y_pred = strategy.experimental_run_v2(_train_step, args=(sample[:3], sample[4:6], current_step,))\n",
    "            sum_loss, y_pred = train_step(sample[:3], sample[4:6], current_step)\n",
    "    \n",
    "            # # for i in range(strategy.num_replicas_in_sync):\n",
    "            # #     epoch_loss += loss.values[i]\n",
    "            # epoch_loss += sum_loss\n",
    "            # loss_step.append(epoch_loss/(batch_num+1))\n",
    "            # \n",
    "            # tf.print(\"training ... batch\", batch_num+1, \": \", \"train loss \", epoch_loss/(batch_num+1))\n",
    "\"\"\"\n",
    "\n",
    "def predict(model, strategy, dataset, loss_fn, optimizer, data_len, fold_num):\n",
    "    \n",
    "    @tf.function\n",
    "    def predict_step(inputs):\n",
    "        def _predict_step(inputs):\n",
    "            return model(inputs)\n",
    "        # replica value\n",
    "        y_pred = strategy.experimental_run_v2(_predict_step, args=(inputs,))\n",
    "        return y_pred\n",
    "        \n",
    "    def to_numpy(*args):\n",
    "        out = []\n",
    "        for arg in args:\n",
    "            if arg.dtype == tf.string:\n",
    "                arg = [s.decode('utf-8') for s in arg.numpy()]\n",
    "                out.append(arg)\n",
    "            else:\n",
    "                arg = arg.numpy()\n",
    "                out.append(arg)\n",
    "        return out\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    offset = tf.zeros([0, MAX_SEQUENCE_LENGTH, 2], dtype=tf.dtypes.int32)\n",
    "    text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    pred_start = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n",
    "    pred_end = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n",
    "    \n",
    "    tk0 = tqdm(dataset, total=data_len, desc=\"Validating or Testing: \" + str(fold_num))\n",
    "    \n",
    "    with strategy.scope():\n",
    "        for batch_num, sample in enumerate(tk0):\n",
    "            \n",
    "            # print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n",
    "            \n",
    "            # y_pred = strategy.experimental_run_v2(predict_step, args=(sample[:3],))\n",
    "            y_pred = predict_step(sample[:3])\n",
    "            \n",
    "            # add batch to accumulators\n",
    "            for i in range(strategy.num_replicas_in_sync):\n",
    "                pred_start = tf.concat((pred_start, y_pred[0].values[i]), axis=0)\n",
    "                pred_end = tf.concat((pred_end, y_pred[1].values[i]), axis=0)\n",
    "                \n",
    "            offset = tf.concat((offset, sample[3]), axis=0)\n",
    "            text = tf.concat((text, sample[6]), axis=0)\n",
    "            selected_text = tf.concat((selected_text, sample[7]), axis=0)\n",
    "            sentiment = tf.concat((sentiment, sample[8]), axis=0)\n",
    "\n",
    "    # pred_start = tf.nn.softmax(pred_start)\n",
    "    # pred_end = tf.nn.softmax(pred_end)\n",
    "    \n",
    "    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n",
    "    \n",
    "    return pred_start, pred_end, text, selected_text, sentiment, offset\n",
    "\n",
    "\n",
    "def decode_prediction(pred_start, pred_end, text, offset, sentiment, is_testing):\n",
    "        \n",
    "    def get_best_start_end_idxs(start_logits, end_logits):\n",
    "        max_len = len(start_logits)\n",
    "        a = np.tile(start_logits, (max_len, 1))\n",
    "        b = np.tile(end_logits, (max_len, 1))\n",
    "        c = np.tril(a + b.T, k=0).T\n",
    "        c[c == 0] = -1000\n",
    "        return np.unravel_index(c.argmax(), c.shape)\n",
    "    \n",
    "    def decode(pred_start, pred_end, text, offset):\n",
    "\n",
    "        decoded_text = \"\"\n",
    "        for i in range(pred_start, pred_end+1):\n",
    "            decoded_text += text[offset[i][0]:offset[i][1]]\n",
    "            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n",
    "                decoded_text += \" \"\n",
    "        return decoded_text\n",
    "    \n",
    "    decoded_predictions = []\n",
    "    for i in range(len(text)):\n",
    "        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n",
    "            decoded_text = text[i]\n",
    "        else:\n",
    "            if not is_testing:\n",
    "                idx_start = np.argmax(pred_start[i])\n",
    "                idx_end = np.argmax(pred_end[i])\n",
    "            else:\n",
    "                idx_start, idx_end = get_best_start_end_idxs(pred_start[i], pred_end[i])\n",
    "            if idx_start > idx_end:\n",
    "                idx_end = idx_start \n",
    "            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n",
    "            if len(decoded_text) == 0:\n",
    "                decoded_text = text[i]\n",
    "        decoded_predictions.append(decoded_text)\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "@tf.function\n",
    "def learning_rate_decay(init_lr, num_train_steps, num_warmup_steps, current_step):\n",
    "    # Implements linear decay of the learning rate.\n",
    "    learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                    init_lr, num_train_steps, end_learning_rate=0.0, power=1.0)(current_step)\n",
    "\n",
    "    if num_warmup_steps:\n",
    "        global_steps_int = tf.cast(current_step, tf.dtypes.int32)\n",
    "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.dtypes.int32)\n",
    "\n",
    "        global_steps_float = tf.cast(global_steps_int, tf.dtypes.float32)\n",
    "        warmup_steps_float = tf.cast(warmup_steps_int, tf.dtypes.float32)\n",
    "\n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "        if global_steps_int < warmup_steps_int:\n",
    "            learning_rate = warmup_learning_rate\n",
    "        else:\n",
    "            learning_rate = learning_rate\n",
    "        \n",
    "    return learning_rate\n",
    "\n",
    "@tf.function\n",
    "def focal_loss(y_actual, y_pred, label_smoothing=0.15):\n",
    "    # label smoothing\n",
    "    y_actual = tf.cast(y_actual, tf.dtypes.int32)\n",
    "    y_actual_one_hot = tf.one_hot(y_actual, MAX_SEQUENCE_LENGTH, axis=-1)\n",
    "    # y_actual_one_hot = y_actual_one_hot * (1 - label_smoothing) + label_smoothing / MAX_SEQUENCE_LENGTH\n",
    "    \n",
    "    # focal loss\n",
    "    result_reduce = tf.reduce_sum(y_actual_one_hot * y_pred, axis=-1)\n",
    "    custom_loss = - tf.math.pow((1 - result_reduce), 1) * tf.math.log(result_reduce)\n",
    "    custom_loss = custom_loss / global_batch_size\n",
    "    return custom_loss"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "III. Run it all: \n",
    "\n",
    "model.create() -> dataset.create() -> train(train) ->\n",
    "       -> predict(val).decode() -> predict(test).decode() -> submit\n",
    "```"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "num_folds = 5\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "global_batch_size = batch_size * strategy.num_replicas_in_sync\n",
    "learning_rate = 4e-5\n",
    "num_train_steps = int(len(train_df) / batch_size * num_epochs)\n",
    "num_warmup_steps = int(num_train_steps * 0.1)\n",
    "pos_offsets = 1\n",
    "    \n",
    "data_df_5folds = generate_fold_data(train_df, 5)\n",
    "\n",
    "with strategy.scope():\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    lr_decay_fn = learning_rate_decay\n",
    "    loss_fn = focal_loss\n",
    "    config = RobertaConfig.from_json_file(os.path.join(PATH, \"config-roberta-base.json\"))\n",
    "    config.output_hidden_states = True\n",
    "    config.num_labels = 2\n",
    "    model = RoBertQAModel(config=config)\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "        \n",
    "# def run(fold):\n",
    "df_train_fold = data_df_5folds[data_df_5folds.kfold != fold].reset_index(drop=True)\n",
    "df_valid_fold = data_df_5folds[data_df_5folds.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "df_train_fold = pd.concat([df_train_fold, train_df_aug])\n",
    "\n",
    "num_train_batches = len(df_train_fold) // batch_size + int(len(df_train_fold) % batch_size != 0)\n",
    "num_eval_batches = len(df_valid_fold) // batch_size + int(len(df_valid_fold) % batch_size != 0)\n",
    "num_test_batches = len(test_df) // batch_size + int(len(test_df) % batch_size != 0)\n",
    "\n",
    "# initialize test predictions\n",
    "test_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "test_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "    \n",
    "    # with strategy.scope():\n",
    "        # optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        # lr_decay_fn = learning_rate_decay\n",
    "        # optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "        #     optimizer, 'dynamic')\n",
    "    \n",
    "        # config = RobertaConfig(output_hidden_states=True, num_labels=2)\n",
    "        # config = RobertaConfig.from_json_file(os.path.join(PATH, \"config-roberta-base.json\"))\n",
    "        # config.output_hidden_states = True\n",
    "        # config.num_labels = 2\n",
    "        # model = RoBertQAModel(config=config)\n",
    "    \n",
    "        # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        # loss_fn = focal_loss\n",
    "    \n",
    "loss_step = []\n",
    "train_dataset = TweetSentimentDataset.create(\n",
    "    df_train_fold, batch_size, shuffle_buffer_size=2048)\n",
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "valid_dataset = TweetSentimentDataset.create(\n",
    "    df_valid_fold, batch_size, shuffle_buffer_size=-1)\n",
    "valid_dataset = strategy.experimental_distribute_dataset(valid_dataset)\n",
    "test_dataset = TweetSentimentDataset.create(\n",
    "    test_df, batch_size, shuffle_buffer_size=-1)\n",
    "test_dataset = strategy.experimental_distribute_dataset(test_dataset)\n",
    "\n",
    "best_score = float('-inf')\n",
    "for epoch_num in range(num_epochs):\n",
    "    # train for an epoch\n",
    "    # train(model, strategy, train_dataset, loss_fn, optimizer, global_step, loss_step, num_train_batches, fold, lr_decay_fn)\n",
    "    for sample in train_dataset:\n",
    "        train_step(sample[:3], sample[4:6], global_step)\n",
    "        \n",
    "        # # predict validation set and compute jaccardian distances\n",
    "        # pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "        #     predict(model, strategy, valid_dataset, loss_fn, optimizer, num_eval_batches, fold)\n",
    "        # \n",
    "        # selected_text_pred = decode_prediction(\n",
    "        #     pred_start, pred_end, text, offset, sentiment, is_testing=False)\n",
    "        # \n",
    "        # # tackle the noise\n",
    "        # for s in range(len(selected_text_pred)):\n",
    "        #     if len(str(selected_text_pred[s]).split())==1:\n",
    "        #         selected_text_pred[s] = post_process(selected_text_pred[s])\n",
    "        #         \n",
    "        # jaccards = []\n",
    "        # for i in range(len(selected_text)):\n",
    "        #     jaccards.append(jaccard(selected_text[i], selected_text_pred[i]))\n",
    "        # \n",
    "        # score = np.mean(jaccards)\n",
    "        # \n",
    "        # if epoch_num + 1 == num_epochs:\n",
    "        #     plt.plot(list(range(global_step.numpy())), loss_step)\n",
    "        #     plt.show()\n",
    "        # print(\"fold = %d , epoch = %d , jaccard = %f\" % (fold, epoch_num+1, score))\n",
    "        # \n",
    "        # if score > best_score:\n",
    "        #     best_score = score\n",
    "        #     # requires you to have 'fold-{fold_num}' folder in PATH:\n",
    "        #     # model.save_pretrained(PATH+f'fold-{fold_num}')\n",
    "        #     # or\n",
    "        #     # model.save_weights(PATH + f'fold-{fold_num}.h5')\n",
    "        # \n",
    "        #     # predict test set\n",
    "        #     test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n",
    "        #         predict(model, strategy, test_dataset, loss_fn, optimizer, num_test_batches, fold)\n",
    "\n",
    "    # # add epoch's best test preds to test preds arrays\n",
    "    # test_preds_start += test_pred_start\n",
    "    # test_preds_end += test_pred_end\n",
    "\n",
    "    # # reset model, as well as session and graph (to avoid OOM issues?) \n",
    "    # session = tf.compat.v1.get_default_session()\n",
    "    # graph = tf.compat.v1.get_default_graph()\n",
    "    # del session, graph, model\n",
    "    # tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    # model = RoBertQAModel(config=config)\n",
    "    # return (test_preds_start, test_preds_end, test_text, test_sentiment, test_offset)\n",
    "    \n",
    "# test_result = Parallel(n_jobs=1, backend=\"threading\", verbose=10)(delayed(run)(i) for i in range(num_folds))\n",
    "# Parallel(n_jobs=1, backend=\"threading\", verbose=10)(delayed(run)(i) for i in range(num_folds))\n",
    "\n",
    "# # initialize test predictions\n",
    "# test_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "# test_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "# \n",
    "# for i in range(num_folds):\n",
    "#     test_preds_start += test_result[i][0]\n",
    "#     test_preds_end += test_result[i][1]\n",
    "# \n",
    "# # decode test set and add to submission file\n",
    "# test_text = test_result[0][2]\n",
    "# test_offset = test_result[0][4]\n",
    "# test_sentiment = test_result[0][3]\n",
    "# selected_text_pred = decode_prediction(\n",
    "#     test_preds_start, test_preds_end, test_text, test_offset, test_sentiment, is_testing=True)\n",
    "# \n",
    "# # tackle the noise\n",
    "# submission_df.loc[:, 'selected_text'] = selected_text_pred\n",
    "# submission_df['selected_text'] = submission_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "# submission_df['selected_text'] = submission_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "# submission_df['selected_text'] = submission_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "# submission_df['selected_text'] = submission_df.apply(lambda x: post_process(x['selected_text']) if (len(str(x['selected_text']).split())==1) else x['selected_text'], axis=1)\n",
    "# submission_df.to_csv(\"submission.csv\", index=False)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Training: 0', max=645.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2926383b184c41249e883974bad080d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "training ... batch 645 : train loss 1.983 \n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating or Testing: 0', max=215.0, style=ProgressStyle…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48f946c47c8c47bb817a92973ba26bac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "text": "fold = 0 , epoch = 1 , jaccard = 0.548545\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating or Testing: 0', max=111.0, style=ProgressStyle…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b06662e932745dd95e520a5251b3ebe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Training: 0', max=645.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ae5bfd4f51140d7be408c0de3abc8f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "training ... batch 645 : train loss 1.584 \n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating or Testing: 0', max=215.0, style=ProgressStyle…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a396161014174ae8ba818d1e9ef45019"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXhc9X3v8fd3du3yIlvyhjAQCEvMorCEpgGSEqAst09oL7lJm6Th8dMUbpPbtLklyZPe8LS9TdObZiEpdZM2JIVAStkLJKw3kNyYyMZgBwM23hds2Za1bzP63j/OkT2SRtbIljw68uf1PPPMmXN+mvnOsfTxb37zO+eYuyMiItEXK3UBIiIyORToIiIzhAJdRGSGUKCLiMwQCnQRkRkiUaoXnjt3rjc2Npbq5UVEImnVqlX73L2u0LaSBXpjYyPNzc2lenkRkUgys61jbdOQi4jIDKFAFxGZIRToIiIzhAJdRGSGUKCLiMwQCnQRkRlCgS4iMkNELtDfeLuDr/30DfZ19pW6FBGRaSVygb5hbwfffHYjB7r6S12KiMi0ErlANwwAXZdDRGS46AV6kOc4SnQRkXzRC/TwXj10EZHhijo5l5ltATqAHJB196YR2y8DHgY2h6secPfbJ6/M/NeaimcVEYm+iZxt8XJ333eE7S+4+7XHWtD4NIYuIlJI5IZchmgMXURkuGID3YGfmtkqM1s+RptLzOwVM3vCzM4q1MDMlptZs5k1t7S0HFXBh74UVZ6LiAxT7JDLpe6+y8zmAU+Z2evu/rO87auBk9y908yuAR4CThv5JO6+AlgB0NTUdFSRrCF0EZHCiuqhu/uu8H4v8CBw4Yjt7e7eGS4/DiTNbO4k1wqAmcbQRUQKGTfQzazCzKqGloErgXUj2tRbmLRmdmH4vPsnv9y8aYsaQxcRGaaYIZf5wINhXieAe9z9STP7IwB3vxO4EfiUmWWBHuAm96npQ2sMXUSksHED3d03AcsKrL8zb/kO4I7JLa2ww0eKiohIvshNWzR9LSoiUlDkAp1DQy7qo4uI5ItcoB/+UlRERPJFL9A1bVFEpKDIBfphSnQRkXyRC3SdPldEpLDoBbqmLYqIFBS9QNfpc0VECopeoGvaoohIQdEL9PBecS4iMlzkAl0HioqIFBa5QNcYuohIYdEL9EOzXJToIiL5ohfoQwvKcxGRYaIX6EOH/pe4DhGR6aaoQDezLWa21szWmFlzge1mZt80s41m9qqZnT/5pQ6nMXQRkeGKvUg0wOXuvm+MbVcTXBT6NOAi4B/D+0mnMXQRkcIma8jlBuAHHvglUGtmDZP03MPoXC4iIoUVG+gO/NTMVpnZ8gLbFwLb8x7vCNcNY2bLzazZzJpbWlomXi06l4uIyFiKDfRL3f18gqGVW8zsN0dsL3S4z6jMdfcV7t7k7k11dXUTLPVILyUiIkUFurvvCu/3Ag8CF45osgNYnPd4EbBrMgocSedyEREpbNxAN7MKM6saWgauBNaNaPYI8AfhbJeLgTZ33z3p1aJzuYiIjKWYWS7zgQfD+d8J4B53f9LM/gjA3e8EHgeuATYC3cAnpqbcw/PQlegiIsONG+juvglYVmD9nXnLDtwyuaUVdriHrkQXEckXwSNFg3sNoYuIDBe9QNfZFkVECopcoA9RnouIDBe5QNe0RRGRwiIX6EMU5yIiw0Uu0E0HioqIFBS9QNeXoiIiBUUv0HXJIhGRgiIb6Oqhi4gMF71AR5egExEpJHqBrh66iEhB0Qv08F7nchERGS56ga4euohIQZEL9CHKcxGR4SIY6DqySESkkMgFus7lIiJSWNGBbmZxM3vZzB4rsO3jZtZiZmvC282TW2bea03VE4uIRFwxl6Ab8mlgPVA9xvb73P3WYy/pyIYuQacOuojIcEX10M1sEfDbwHentpwiagnvNW1RRGS4Yodcvg58Dhg8QpsPmdmrZna/mS0u1MDMlptZs5k1t7S0TLTW8DmCe/XQRUSGGzfQzexaYK+7rzpCs0eBRnd/F/A0cFehRu6+wt2b3L2prq7uqArW2RZFRAorpod+KXC9mW0B7gWuMLN/y2/g7vvdvS98+M/ABZNaZZ5DPfSpegERkYgaN9Dd/TZ3X+TujcBNwLPu/tH8NmbWkPfweoIvT6eUpi2KiAw3kVkuw5jZ7UCzuz8C/ImZXQ9kgQPAxyenvEKvG9wrzkVEhptQoLv788Dz4fKX8tbfBtw2mYWJiMjERPBIUXXRRUQKiV6gh/eahy4iMlz0Al3z0EVECopeoOsSdCIiBUUv0NVDFxEpKHqBHt5rDF1EZLjIBXosFkT64KACXUQkX+QCPREGek6BLiIyTOQCfaiHnlWgi4gME7lAH+qhD+pbURGRYSIX6DFTD11EpJDIBfqhMfScAl1EJF/kAj0+FOgachERGSZygW5mxEyzXERERopcoEPQS1egi4gMV3Sgm1nczF42s8cKbEub2X1mttHMVppZ42QWOZICXURktIn00D/N2JeW+yTQ6u6nAv8AfOVYCzuSuJlmuYiIjFBUoJvZIuC3ge+O0eQG4K5w+X7g/XboShSTTz10EZHRiu2hfx34HDA4xvaFwHYAd88CbcCckY3MbLmZNZtZc0tLy1GUG1Cgi4iMNm6gm9m1wF53X3WkZgXWjUpcd1/h7k3u3lRXVzeBMoeLx2KatigiMkIxPfRLgevNbAtwL3CFmf3biDY7gMUAZpYAaoADk1jnMPGYDiwSERlp3EB399vcfZG7NwI3Ac+6+0dHNHsE+Fi4fGPYZsoSN6EeuojIKImj/UEzux1odvdHgO8BPzSzjQQ985smqb6C4jEjmxtrOF9E5MQ0oUB39+eB58PlL+Wt7wV+dzILO5KKdILOvuzxejkRkUiI5JGiNWUJ2nsU6CIi+SIa6EnaegZKXYaIyLQSyUCvyiQ15CIiMkIkAz2ViNGX1ZeiIiL5ohno8Rj92VypyxARmVYiGejpRIx+TVsUERkmkoGejMfo15CLiMgwkQz0VCLGoKODi0RE8kQ20AENu4iI5IlmoMfDQNewi4jIIdEMdPXQRURGiXagq4cuInJINANdQy4iIqNEM9A15CIiMko0A109dBGRUYq5pmjGzF4ys1fM7Ndm9uUCbT5uZi1mtia83Tw15QY0hi4iMloxF7joA65w904zSwIvmtkT7v7LEe3uc/dbJ7/E0RToIiKjjRvo4bVBO8OHyfBW0gt6DgV6n8bQRUQOKWoM3cziZrYG2As85e4rCzT7kJm9amb3m9niMZ5nuZk1m1lzS0vLURetMXQRkdGKCnR3z7n7ucAi4EIzO3tEk0eBRnd/F/A0cNcYz7PC3Zvcvamuru6oix7qoff06xS6IiJDJjTLxd0PElwk+qoR6/e7e1/48J+BCyalujEsmV1OJhljzfaDU/kyIiKRUswslzozqw2Xy4APAK+PaNOQ9/B6YP1kFjlSJhlndnmKjl5dhk5EZEgxs1wagLvMLE7wH8CP3f0xM7sdaHb3R4A/MbPrgSxwAPj4VBU8JJOK06urFomIHFLMLJdXgfMKrP9S3vJtwG2TW9qRZRJx+gYU6CIiQyJ5pChAJhmjR4EuInJIhAM9Tu+Api2KiAyJbKCXJeOatigikieygV6ZSdDRN1DqMkREpo3IBvqs8hQHuxToIiJDIh3oHX1ZBnQ+FxERIMKBXl0WzLjs1MFFIiJAhAO9LBkH0MFFIiKhyAZ6ZijQNXVRRASYAYGuqYsiIoEIB3p4Cl0dLSoiAkQ40IfG0HU+FxGRQHQDPRUOuSjQRUSACAf6oTF0BbqICBDhQC/TLBcRkWEiG+jp8EvRh9fspKWjb5zWIiIzXzGXoMuY2Utm9oqZ/drMvlygTdrM7jOzjWa20swap6LYfEM99Bc27OPvnnx9nNYiIjNfMT30PuAKd18GnAtcZWYXj2jzSaDV3U8F/gH4yuSWOdrQGDrA2+29U/1yIiLT3riB7oHO8GEyvPmIZjcAd4XL9wPvNzObtCoLSMYPl/7Chn2s29k2lS8nIjLtFTWGbmZxM1sD7AWecveVI5osBLYDuHsWaAPmFHie5WbWbGbNLS0tx1b5CNd+60UefWXXpD6niEiUFBXo7p5z93OBRcCFZnb2iCaFeuMje/G4+wp3b3L3prq6uolXO45/ePpN3Ee9rIjICSExkcbuftDMngeuAtblbdoBLAZ2mFkCqAEOTFaRY/k/v7uM3KBz4wWLuPdX2/n8g2t5afMBLlo66sOBiMiMV8wslzozqw2Xy4APACOnlTwCfCxcvhF41o9DV/lDFyzi9969mFjMuP7cBVSmEzz48s6pflkRkWmpmB56A3CXmcUJ/gP4sbs/Zma3A83u/gjwPeCHZraRoGd+05RVPIbKdIJzFtbw+tsdx/ulRUSmhXED3d1fBc4rsP5Lecu9wO9ObmkTd0ZDFfe+tJ1sbpBEPLLHTImIHJUZlXrvbpxNz0COFzbsK3UpIiLH3YwK9Pe/cx5zK1M8oHF0ETkBzahATyfiXH76PB59ZRftvQOlLkdE5LiaUYEOcOVZ9QBc960X6c/qTIwicuKYcYH+gXfO48oz57N1fzdff/rNUpcjInLczLhANzNW/EETHzp/Ed95/i3+9+PrS12SiMhxMeMCfcjnrjodgH/62Sb+6rHXSlyNiMjUm7GBPr86w4a/vprLTq/juy9u5sl1u0tdkojIlJqxgQ7BKXY/+1tBT/2P/m01tz2wtsQViYhMnRkd6ADnLKrhmc++D4AfvbSNm+9q1uwXEZmRZnygA5xSV8lrt3+QdzZU8/T6PXz+QfXURWTmmdDpc6OsPJXgiU+/l1vvWc39q3YwkBvkyjPrueaceqb44koiIsfFCdFDz/fl688C4OE1u7jlntXces/LDOQ0BCMi0XfC9NCHzKlM8+Rn3sumli6ee30v/75qB+lkjK/93rmlLk1E5JiccIEOcEZ9NWfUV3P12fWs3tbKA6t38su39nP7DWfzvtPrhl2AWkQkKoq5YtFiM3vOzNab2a/N7NMF2lxmZm1mtia8fanQc003ZsZ//sl7ufXyU8m5c/MPmnnf3z3HM+v3lLo0EZEJK6YrmgU+6+7vBC4GbjGzMwu0e8Hdzw1vt09qlVMok4zzZx88nef+7DL+7Mp30NWf45N3NfPdFzaVujQRkQkZN9Ddfbe7rw6XO4D1wMKpLux4K08luPWK0/jZn1/OssW1/M3j67nj2Q2lLktEpGgTGiw2s0aCy9GtLLD5EjN7xcyeMLOzxvj55WbWbGbNLS0tEy72eKgpT3LPzRdxxRnz+PufvskN3/4563a2lbosEZFxFR3oZlYJ/AfwGXdvH7F5NXCSuy8DvgU8VOg53H2Fuze5e1NdXd3R1jzlKtIJvv2R8/mvTYvZsKeD3/nOz/naU2+S1fRGEZnGigp0M0sShPnd7v7AyO3u3u7uneHy40DSzOZOaqXHWToR5ys3votnPvs+PnhWPd98ZgM33vn/2HWwp9SliYgUVMwsFwO+B6x396+N0aY+bIeZXRg+7/7JLLRUGmrKuOO/nc+3Pnweb+3t5D1/+yxfeHAtbT26xJ2ITC/FzEO/FPh9YK2ZrQnXfR5YAuDudwI3Ap8ysyzQA9zk7j4F9ZbMdcsW0FCTYfkPV3H3ym08/0YLX/ztd3L1OQ2lLk1EBAArVe42NTV5c3NzSV77WOQGnWdf38tf/+drbNnfTX11hnuXX0zj3IpSlyYiJwAzW+XuTQW3KdCPTjY3yGfuW8Njr+4mGTfetaiWZYtq+f1LTuJkhbuITBEF+hR6q6WTbz+7kQ17O1m7s42KVJwbzlvIRy86iTMXVJe6PBGZYRTox8nrb7fzlSde55ebDtAzkKMyneC6ZQ1ct2wBlyydo9P0isgxU6AfZwe6+vnOcxvZsr+LX7y1n+7+HO9aVMNf/Zezedei2lKXJyIRpkAvoe7+LI+9upuv/uQNWjr6OKO+ive9o453N87mstPrSOjMjiIyAQr0aaCtZ4B7Vm7jP9fuYt3O4EDbBTUZbmxazDXn1HNGvcbbRWR8CvRpZndbD6u2tnLPym384q3g+KvGOeWcv2QWV55VT1PjLOZWpktcpYhMRwr0aWzr/i4eX/s2L29r5Zeb9tPemyUeM95zyhwuPXUuJ8+t4PT5VZrnLiKAAj0yegdyrN7ayosb9/HEurfZvK/r0LbGOeWctaCG85bUcu7iWs5ZVEM6ES9htTNXNjdIPGaalSTTkgI9otq6B9iyv4uXNh/g5e2trNrayp72PgDSiRhXnlXPeYtrufTUuSycVUZl+oS8ouCk6svmOP2LT3LL5afw5x88o9TliIxypEBXAkxjNeVJlpXXsmxxMNXR3Wnp6GPV1laeWr+HZ9bv5dFXdgGQiBn1NRmW1lXyifc0cvbCGuZWptTLnKCuvhwAd6/cpkCXyFGgR4iZMa86w9XnNHD1OQ0MDjrbDnSzelsrb+7pZPuBblZuPsAnvv8rIOjFz6tO0zinguqyJLVlSU6vr+KUukqWzC5n8ezyEr+j6Sc7GJzzPhHTf4QSPQr0CIvFjMa5FcO+MO3PDvKzN1vYsr+LbQe6ae0eYOv+LnYe7GFfRx93r8weattQk+GCk2axcFYZ86oy1JYlKUvFOXVeJfOrM1RnEidcDz83GAxBxk6w9y0zgwJ9hkklYnzgzPkFt7k7b7f3snlfF+t3d7B6aytrd7bx01/vob/A1ZhS8RhzKlPMrUwzpzJFZTrBwlllLJldfui2oLaM5Aw6OCqbCwJdPXSJIgX6CcTMaKgpo6GmjPecMpdP/sbJQBD07T1ZWjr76OnPsWlfJy0dfezr7Gd/Zx/7OoPlLfu6RoV/PGYsqM2wZHY5dZVp6qrSLKgto6Emw5zKNItmlVFfnYlMT3/ovcUU6BJB4wa6mS0GfgDUA4PACnf/xog2BnwDuAboBj7u7qsnv1yZCmZGTXmSmvIkAOcsqhmz7eCgs6ejl637u9l2oJvtB4L74HEre9v76MsO7+0n40YyHqOmLEl1Jkl1WYKasiTlqQRVmQQLasuYV5VmYW0ZVZkk86rT1FWmSxKqA2GgxxXoEkHF9NCzwGfdfbWZVQGrzOwpd38tr83VwGnh7SLgH8N7mWFiscO9/IuXzhm13d050NXPnvagZ7/tQDc7WnsYyA3S3jNAe+8AbT0D7DrYS3d/lraeAVq7R1/OLxWP0VCbYUFNMMTzjvoqqjIJasuSNNSUccq8CspTk/8BcyAbDLnEI/KJQiTfuH8R7r4b2B0ud5jZemAhkB/oNwA/CC8790szqzWzhvBn5QRiZsypTDNnAqcu6OnP0dLRx/bWbjr7suxt72XnwV52Huxh18Eenl6/h/uat4/6ufrqDCfPrWDx7DKqM0kq0gkq0nHmVWWYVZFi8awy6msyEwr+/lwwbVFDLhJFE+rimFkjcB6wcsSmhUD+X9yOcJ0CXcZVloqzZE45S+YUnkY51Ovv7s/R1jPAjtZu3mrpYlNLF1v2d/F/32yhozdLd3+u4M9XpRPMr8mwsLaMxjnlLJpVTioRwwxmV6RoqMmQTsRJJWK88XYnoC9FJZqKDnQzqwT+A/iMu7eP3FzgR0Ydgmpmy4HlAEuWLJlAmXIiO9TrBxYDZy8sPMbv7kEPv6OP/Z397GjtZk97H3vae9nT3sv21m5WbW2lsy9b8Ofz1YbfJ4hESVGBbmZJgjC/290fKNBkB8Hf2pBFwK6Rjdx9BbACgkP/J1ytyBGYGVWZJFWZJKfUwYUnzx7Vxt1p782SzQ0y6NDS0ceejl76s4MM5Abpzw7ypz9+haV1lSV4ByLHpphZLgZ8D1jv7l8bo9kjwK1mdi/Bl6FtGj+X6cjMqCk73Puuq0pzJsPPRf+3T7zO4KD6GxI9xfTQLwV+H1hrZmvCdZ8HlgC4+53A4wRTFjcSTFv8xOSXKnJ8xGN26IhRkSgpZpbLixQeI89v48Atk1WUSCnFzFCeSxTNnGO2RSZJLAaDJTqttMixUKCLFNDdP/5MGJHpRudyERlh+4Eeth/o4epvvEBfNoc7PPrff0MXEJFpTz10kTGs393OppYuNu/r4g+//yvW7mjjlntW0zPGAUwipaYuh0gRXtp8gOvueBGAMxuqueXyU0tckchoCnSRcXz6/afxjWc2HHr81Z+8wVd/8gYAf3ndmbR09LGppYu//71lGpaRktKQi8gRfOOmc/njy08BYH51mjs/esGw7V9+9DW+8/xbPPnrt7n2my+wt72X59/Yy9effpNNLZ2lKFlOYOYlmp7V1NTkzc3NJXltkSP5/s83878eDU4m+q+feDeXnz5v2PZNLZ3s6+ynpizJ42t3k07GeHHDPn7x1v5h7eIxY1Z5is984DRuvGARmWT8uL0HmbnMbJW7NxXaps+HIiNsPdB9aLk6M/okXUvrKllaFyyfXl8FwB9eejLffGYD33n+LaoyCf76d87h5xv2cV/zdr740Dq++NA6TptXSVkqTlvPALMrUry87SCLZ5fxofMXccFJs7hk6RwS4eX8Xt1xkPaeLJWZBMsW1WBmDOQG2d/Zj+Psbe+jZyBHRSrB/Jo0Vekkrd39JOLBqQ3SCf3ncSJSoIuMkH/FpfzzvhxJJhnnc1edwcfe00hVJkF5KsH1yxbwpevO5OE1u/j+Lzbz5p5gCKa+OsPaHW1AMEXyG89swD24qMd7T5tLe+8Av9rSeui5z1tSSzIe46XNB4p+D7MrUlRnEvRlB+nqy1JdliRmRjoRY1Z5ivNPmkV1WYJUPMZZC2qCc8qXJalMJXQu+AjTkIvICHvae7nob57hwpNn88NPXjhpvd1nX99DzIzLTp/H0N/doMO+zj4efWUXD63ZyYY9nZw8t4LffEcd726czbYD3dz1iy3EY0ZdZZr3v3Me2UGnuizJ4lll9GUHWbW1lXQixvzqDD39ObYd6Kajd4CcB+fsqC5L0N2XY9Cdtp4BDnT1s25Xe8Hz1cQsOGFZWTJOZSbB/KoMDbUZ6qszzK/OUF8TLM+pTNOXzTGvKqPL9R1nRxpyUaCLnIAGcoPkBp2323p5dWcbPf1Z2nuytHb3s68zuC5se88AW/Z309rdz8EClwkEKEvGOaOhigW1ZfSHn2xmlSeZVZEiFY+RisdYOKuMk+aUU5YMriFblUlQmU6QiMdwd9x1haiJ0Bi6iAyTjMdIxqFxbgWNcyvGbd87kGNPey9vt/XydnsvLR19pBIx1u/uYMu+Ll7b1U46EaM/O0hnX5YDXf0Muh/xJGcVqTg5dwYHob4mgxMsz6tOs2R2OafUVWJAPB6E/eBg8HzZQSebG2R2RYryVIJMMkZ5Ko47zKkMPl2kErHDt3hwnw6Xc2EnNhmfeZP8FOgiMq5MMs5Jcyo4ac744Q/BhUTMjL5sjo17O9nb3kdfNkd7b5aO3iwdvQN09GaDwI4Zu9p6ScQMd6els4/mLa08vGbUNXIOmYxTHJengqG0dCJGeSr41FCZSTCrPMXQB4be7CAD2eDTTHZwkLJUnLJkgupMgvJ0nHQizpzKFFXpRHhN2+B5gvv44cfH6bsJBbqITLrgujiQTsQ5a0ENZy2Y+HP09OewvAyMmREPQz8eM9p7svQM5OgdyB26nuz+rj56B4IrT/XncsF9dpC+7CD94RWp4mY4cLB7ALNg+KmzL0tXX/CfzY7Ww7Oc0sk46XiMeMxIJRLhp48B1vcM0DOQo6c/R89AcaeCKE8dDviPXLSEm9+7dOI7ZRwKdBGZlspSY30ZHaR8TXmSGkp77Vd3p3fg8H8IQ/dd/Vk6+3J09hZeP7cyPSX1FHMJun8BrgX2uvvZBbZfBjwMbA5XPeDut09mkSIi05GZBcMwqTh1VVMT0hNRTA/9+8AdwA+O0OYFd792UioSEZGjMu7XvO7+M6D4IxpERKQkJmveziVm9oqZPWFmZ43VyMyWm1mzmTW3tLRM0kuLiAhMTqCvBk5y92XAt4CHxmro7ivcvcndm+rq6ibhpUVEZMgxB7q7t7t7Z7j8OJA0s7nHXJmIiEzIMQe6mdVbOOnUzC4Mn3P/kX9KREQmWzHTFn8EXAbMNbMdwF9CMPnT3e8EbgQ+ZWZZoAe4yUt1ghgRkRPYuIHu7h8eZ/sdBNMaRUSkhEp2tkUzawG2HuWPzwX2TWI5x1uU61ftpRHl2iHa9U+32k9y94KzSkoW6MfCzJrHOn1kFES5ftVeGlGuHaJdf5Rqn3nnjxQROUEp0EVEZoioBvqKUhdwjKJcv2ovjSjXDtGuPzK1R3IMXURERotqD11EREZQoIuIzBCRC3Qzu8rM3jCzjWb2F6WuZyQzW2xmz5nZejP7tZl9Olw/28yeMrMN4f2scL2Z2TfD9/OqmZ1f2ncAZhY3s5fN7LHw8clmtjKs/T4zS4Xr0+HjjeH2xhLXXWtm95vZ6+H+vyRi+/1/hL8z68zsR2aWma773sz+xcz2mtm6vHUT3tdm9rGw/QYz+1gJa/9q+Hvzqpk9aGa1edtuC2t/w8w+mLd++mWRu0fmBsSBt4ClQAp4BTiz1HWNqLEBOD9crgLeBM4E/g74i3D9XwBfCZevAZ4guK7WxcDKafAe/hS4B3gsfPxjglM6ANwJfCpc/mPgznD5JuC+Etd9F3BzuJwCaqOy34GFBFf9Ksvb5x+frvse+E3gfGBd3roJ7WtgNrApvJ8VLs8qUe1XAolw+St5tZ8Z5kwaODnMn/h0zaKSvvhR/ENcAvwk7/FtwG2lrmucmh8Gfgt4A2gI1zUAb4TL/wR8OK/9oXYlqncR8AxwBfBY+Ee4L++X/dC/AfAT4JJwORG2sxLVXR0Goo1YH5X9vhDYHoZbItz3H5zO+x5oHBGKE9rXwIeBf8pbP6zd8ax9xLbfAe4Ol4dlzNB+n65ZFLUhl6Ff+iE7wnXTUvgx+DxgJTDf3XcDhPfzwmbT7T19HfgcMBg+ngMcdPds+Di/vkO1h9vbwvalsBRoAf41HC76rplVEJH97u47gb8HtgG7CfblKqKx74dMdF9Pq3+DPH9I8IkCIlZ71ALdCqyblvMuzawS+A/gM+7efqSmBdaV5D2Z2dDFwFflry7Q1IvYdrwlCD5G/1JSXvYAAAIcSURBVKO7nwd0EXzsH8t0qp1wvPkGgo/1C4AK4OoCTafjvh/PWLVOu/dgZl8AssDdQ6sKNJuWtUP0An0HsDjv8SJgV4lqGZOZJQnC/G53fyBcvcfMGsLtDcDecP10ek+XAteb2RbgXoJhl68DtWY2dGbO/PoO1R5ur6F015/dAexw95Xh4/sJAj4K+x3gA8Bmd29x9wHgAeA9RGPfD5novp5W/wbhl7LXAh/xcByFiNQ+JGqB/ivgtPCb/xTBl0GPlLimYczMgO8B6939a3mbHgGGvsX/GMHY+tD6PwhnAlwMtA19bD3e3P02d1/k7o0E+/ZZd/8I8BzBee9hdO1D7+nGsH1Jeinu/jaw3cxOD1e9H3iNCOz30DbgYjMrD3+Hhuqf9vs+z0T39U+AK81sVvgJ5cpw3XFnZlcB/xO43t278zY9AtwUzio6GTgNeInpmkWlHsQ/ii8zriGYOfIW8IVS11Ogvt8g+Oj1KrAmvF1DML75DLAhvJ8dtjfg2+H7WQs0lfo9hHVdxuFZLksJfok3Av8OpMP1mfDxxnD70hLXfC7QHO77hwhmTkRmvwNfBl4H1gE/JJhZMS33PfAjgrH+AYLe6iePZl8TjFdvDG+fKGHtGwnGxIf+Zu/Ma/+FsPY3gKvz1k+7LNKh/yIiM0TUhlxERGQMCnQRkRlCgS4iMkMo0EVEZggFuojIDKFAFxGZIRToIiIzxP8Homb+1zWLIQgAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "text": "fold = 0 , epoch = 2 , jaccard = 0.567901\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating or Testing: 0', max=111.0, style=ProgressStyle…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b14c7848c1b43d7a0c7142aa19af7d3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Training: 0', max=645.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97d1d0625b3c4439afe5aed562a2a131"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "training ... batch 645 : train loss 1.411 \n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating or Testing: 0', max=215.0, style=ProgressStyle…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98e66c697e3c4415ada6b7b7554065f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "text": "fold = 0 , epoch = 3 , jaccard = 0.570380\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating or Testing: 0', max=111.0, style=ProgressStyle…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49fcd5a826a2476d89569a307ca3c82a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'BertQAModel' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6a73b654eadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_preds_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"threading\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# initialize test predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6a73b654eadc>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertQAModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_preds_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertQAModel' is not defined"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}